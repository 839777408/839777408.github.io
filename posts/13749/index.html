<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Kafka | Nan</title><meta name="author" content="阿楠"><meta name="copyright" content="阿楠"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="参考视频：【尚硅谷】2022版Kafka3.x教程 https:&#x2F;&#x2F;www.bilibili.com&#x2F;video&#x2F;BV1vr4y1677k  概述定义Kafka传统定义：Kafka是一个分布式的基于发布&#x2F;订阅模式的消息队列（Message Queue），主要应用于大数据实时处理领域。 发布&#x2F;订阅：消息的发布者不会将消息直接发送给特定的订阅者，而是将发布的消息分为不同的类别，订">
<meta property="og:type" content="article">
<meta property="og:title" content="Kafka">
<meta property="og:url" content="https://nanzx.gitee.io/posts/13749/index.html">
<meta property="og:site_name" content="Nan">
<meta property="og:description" content="参考视频：【尚硅谷】2022版Kafka3.x教程 https:&#x2F;&#x2F;www.bilibili.com&#x2F;video&#x2F;BV1vr4y1677k  概述定义Kafka传统定义：Kafka是一个分布式的基于发布&#x2F;订阅模式的消息队列（Message Queue），主要应用于大数据实时处理领域。 发布&#x2F;订阅：消息的发布者不会将消息直接发送给特定的订阅者，而是将发布的消息分为不同的类别，订">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://unpkg.com/nan-picture/img/wp7.jpg">
<meta property="article:published_time" content="2022-02-25T18:09:36.000Z">
<meta property="article:modified_time" content="2022-03-19T23:56:42.000Z">
<meta property="article:author" content="阿楠">
<meta property="article:tag" content="Kafka">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://unpkg.com/nan-picture/img/wp7.jpg"><link rel="shortcut icon" href="https://unpkg.com/nan-picture/img/fav.jpg"><link rel="canonical" href="https://nanzx.gitee.io/posts/13749/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Kafka',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-03-19 23:56:42'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="./css/iconfont.css"><link rel="stylesheet" href="./css/background.css"><link rel="stylesheet" href="./css/ahzoo.css"><link rel="stylesheet" href="./css/aplayer.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://unpkg.com/nan-picture/img/head.jpg" onerror="onerror=null;src='https://unpkg.com/nan-picture/img/load.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">83</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">104</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="https://nanzx.gitee.io"><i class="fa-fw fas fa-rocket"></i><span> 镜像</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Nan"><span class="site-name">Nan</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="https://nanzx.gitee.io"><i class="fa-fw fas fa-rocket"></i><span> 镜像</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Kafka</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-02-25T18:09:36.000Z" title="发表于 2022-02-25 18:09:36">2022-02-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-03-19T23:56:42.000Z" title="更新于 2022-03-19 23:56:42">2022-03-19</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Kafka/">Kafka</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">17k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>66分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Kafka"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/posts/13749/#post-comment"><span id="twikoo-count"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div><article class="post-content" id="article-container"><blockquote>
<p>参考视频：【尚硅谷】2022版Kafka3.x教程 <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1vr4y1677k">https://www.bilibili.com/video/BV1vr4y1677k</a></p>
</blockquote>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>Kafka传统定义：Kafka是一个<strong>分布式</strong>的基于<strong>发布&#x2F;订阅模式</strong>的<strong>消息队列</strong>（Message Queue），主要应用于大数据实时处理领域。</p>
<p>发布&#x2F;订阅：消息的发布者不会将消息直接发送给特定的订阅者，而是<strong>将发布的消息分为不同的类别</strong>，订阅者<strong>只接收感兴趣的消息</strong>。</p>
<p>Kafka最新定义： Kafka是一个开源的<strong>分布式事件流平台</strong>（Event Streaming Platform），被数千家公司用于高性能<strong>数据管道、流分析、数据集成和关键任务应用</strong>。</p>
<h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p><strong>缓冲&#x2F;消峰</strong>：有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况</p>
<p><strong>解耦：</strong>允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。</p>
<p><strong>异步通信：</strong>允许用户把一个消息放入队列，但并不立即处理它，然后在需要的时候再去处理它们。</p>
<h2 id="消息队列的两种模式"><a href="#消息队列的两种模式" class="headerlink" title="消息队列的两种模式"></a>消息队列的两种模式</h2><ul>
<li>点对点模式（一个生产者，一个消费者）<ul>
<li>消费者主动拉取数据，消息收到后清除消息</li>
</ul>
</li>
</ul>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220225185206.png"></p>
<ul>
<li>发布订阅模式（一个生产者，多个消费者）<ul>
<li>可以有多个topic主题（浏览、点赞、收藏、评论等）</li>
<li>消费者消费数据之后，不删除数据</li>
<li>每个消费者相互独立，都可以消费到数据</li>
</ul>
</li>
</ul>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220225185325.png"></p>
<h2 id="基础架构"><a href="#基础架构" class="headerlink" title="基础架构"></a>基础架构</h2><p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220226105508.png"></p>
<ul>
<li><p>Broker：代理；一台Kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个 topic。 </p>
</li>
<li><p>Topic：主题；可以理解为一个队列，<strong>生产者和消费者面向的都是一个topic</strong>。</p>
</li>
<li><p>Producer：消息生产者；就是向 Kafka broker 发消息的客户端。 </p>
</li>
<li><p>Consumer：消息消费者；向 Kafka broker 取消息的客户端。 </p>
</li>
<li><p>Partition：分区；为了实现扩展性和提高吞吐量，一个非常大的 topic 可以分布到多个 broker（即服务器）上，一个 topic 可以分为多个 partition，每个 partition 是一个<strong>有序</strong>的队列。 </p>
</li>
<li><p>Consumer Group（CG）：消费者组；由多个 consumer 组成。消费者组内每个消费者负责消费不同分区的数据，<strong>一个分区只能由组内一个消费者消费，组内每个消费者并行消费</strong>；<strong>消费者组之间互不影响</strong>。所有的消费者都属于某个消费者组，即<strong>消费者组是逻辑上的一个订阅者</strong>。 </p>
</li>
<li><p>Replica：副本。一个 topic 的每个分区都有若干个副本，一个 Leader 和若干个Follower。 </p>
</li>
<li><p>Leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是 Leader。 </p>
</li>
<li><p>Follower：每个分区多个副本中的“从”，实时从 Leader 中同步数据，保持和Leader 数据的同步。Leader 发生故障时，某个 Follower 会成为新的 Leader，保证服务的可用性和数据一致性。</p>
</li>
<li><p>Offset：消息在对应 Topic 中的偏移量</p>
</li>
</ul>
<blockquote>
<p>注：</p>
<ul>
<li><p>Replica（Leader\Follower）对应的是分区 Partition，Partition(Partition-0\Partition-1)对应的是Topic</p>
</li>
<li><p>Zookeeper 中会记录整个集群中那些 broker 可用&#x2F;上线【&#x2F;brokers&#x2F;ids[0,1,2]】，也会记录每一个 partition 中的 leader 信息【&#x2F;brokers&#x2F;topics&#x2F;主题名称&#x2F;partition&#x2F;0&#x2F;state】</p>
</li>
<li><p>2.8.0 之后也可以不配置 Zookeeper 作为注册中心，未来的趋势也是去 Zookeeper  化</p>
</li>
</ul>
</blockquote>
<h1 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h1><h2 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h2><p>集群规划：</p>
<table>
<thead>
<tr>
<th>服务器名称</th>
<th>hadoop102</th>
<th>hadoop103</th>
<th>hadoop104</th>
</tr>
</thead>
<tbody><tr>
<td>依赖的服务</td>
<td>ZooKeeper</td>
<td>ZooKeeper</td>
<td>ZooKeeper</td>
</tr>
<tr>
<td>服务</td>
<td>Kafka</td>
<td>Kafka</td>
<td>Kafka</td>
</tr>
</tbody></table>
<p>官网下载链接，选择Binary downloads： <a target="_blank" rel="noopener" href="https://kafka.apache.org/downloads.html">https://kafka.apache.org/downloads.html</a></p>
<p>Kafka是由Scala语言编写，<a target="_blank" rel="noopener" href="https://www.apache.org/dyn/closer.cgi?path=/kafka/3.1.0/kafka_2.12-3.1.0.tgz">kafka_2.12-3.1.0.tgz</a>代表是使用scala 2.12 版本编写的3.1.0版本的Kafka</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">上传至服务器并解压</span></span><br><span class="line">[root@nanzx opt]# tar -zxvf kafka_2.12-3.1.0.tgz -C /opt</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">修改解压后的文件名称</span></span><br><span class="line">[root@nanzx opt]# mv kafka_2.12-3.1.0/ kafka</span><br></pre></td></tr></table></figure>

<p>进入config目录修改配置文件 server.properties（主要是broker.id&#x2F;log.dirs&#x2F;zookeeper.connect）：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#broker的全局唯一编号，不能重复，只能是数字。</span></span><br><span class="line"><span class="attr">broker.id</span>=<span class="string">0</span></span><br><span class="line"><span class="comment">#处理网络请求的线程数量</span></span><br><span class="line"><span class="attr">num.network.threads</span>=<span class="string">3</span></span><br><span class="line"><span class="comment">#用来处理磁盘 IO 的线程数量</span></span><br><span class="line"><span class="attr">num.io.threads</span>=<span class="string">8</span></span><br><span class="line"><span class="comment">#发送套接字的缓冲区大小</span></span><br><span class="line"><span class="attr">socket.send.buffer.bytes</span>=<span class="string">102400</span></span><br><span class="line"><span class="comment">#接收套接字的缓冲区大小</span></span><br><span class="line"><span class="attr">socket.receive.buffer.bytes</span>=<span class="string">102400</span></span><br><span class="line"><span class="comment">#请求套接字的缓冲区大小</span></span><br><span class="line"><span class="attr">socket.request.max.bytes</span>=<span class="string">104857600</span></span><br><span class="line"><span class="comment">#kafka运行日志(数据)存放的路径，路径不需要提前创建，kafka自动帮你创建，默认是/temp路径下，会被自动清理</span></span><br><span class="line"><span class="comment">#可以配置多个磁盘路径，路径与路径之间可以用&quot;，&quot;分隔，</span></span><br><span class="line"><span class="attr">log.dirs</span>=<span class="string">/opt/kafka/datas</span></span><br><span class="line"><span class="comment">#topic 在当前 broker 上的分区个数</span></span><br><span class="line"><span class="attr">num.partitions</span>=<span class="string">1</span></span><br><span class="line"><span class="comment">#用来恢复和清理 data 下数据的线程数量</span></span><br><span class="line"><span class="attr">num.recovery.threads.per.data.dir</span>=<span class="string">1</span></span><br><span class="line"><span class="comment"># 每个 topic 创建时的副本数，默认是 1 个副本</span></span><br><span class="line"><span class="attr">offsets.topic.replication.factor</span>=<span class="string">1</span></span><br><span class="line"><span class="comment">#segment文件保留的最长时间，超时将被删除</span></span><br><span class="line"><span class="attr">log.retention.hours</span>=<span class="string">168</span></span><br><span class="line"><span class="comment">#每个 segment 文件的大小，默认最大 1G</span></span><br><span class="line"><span class="attr">log.segment.bytes</span>=<span class="string">1073741824</span></span><br><span class="line"><span class="comment">#检查过期数据的时间，默认 5 分钟检查一次是否数据过期</span></span><br><span class="line"><span class="attr">log.retention.check.interval.ms</span>=<span class="string">300000</span></span><br><span class="line"><span class="comment">#配置连接 Zookeeper 集群地址（在 zk 根目录下创建/kafka，方便管理）</span></span><br><span class="line"><span class="attr">zookeeper.connect</span>=<span class="string">hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka</span></span><br></pre></td></tr></table></figure>

<p>配置环境变量<code>vi /etc/profile</code>：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#KAFKA_HOME</span></span><br><span class="line"><span class="attr">export</span> <span class="string">KAFKA_HOME=/opt/kafka</span></span><br><span class="line"><span class="attr">export</span> <span class="string">PATH=$PATH:$KAFKA_HOME/bin</span></span><br></pre></td></tr></table></figure>

<p>使环境变量生效：<code>source /etc/profile</code></p>
<p>分发安装包以及环境变量文件，通过【<a target="_blank" rel="noopener" href="https://blog.csdn.net/nalw2012/article/details/98322637">xsync同步脚本</a>】将文件分发到其他服务器上，注意修改broker.id，一个集群环境中不能重复。</p>
<h2 id="启动和停止"><a href="#启动和停止" class="headerlink" title="启动和停止"></a>启动和停止</h2><ul>
<li>先启动 Zookeeper 集群，然后启动 Kafka<ul>
<li><code>zk.sh start</code></li>
</ul>
</li>
</ul>
<ul>
<li><p>单节点启动Kafka，需依次在 hadoop102、hadoop103、hadoop104 节点上执行：</p>
<ul>
<li>-daemon 表示守护线程启动，再指定一个配置文件的路径</li>
<li><code>bin/kafka-server-start.sh -daemon config/server.properties</code></li>
</ul>
</li>
<li><p>单节点关闭Kafka，需依次在 hadoop102、hadoop103、hadoop104 节点上执行：</p>
<ul>
<li><code>bin/kafka-server-stop.sh</code></li>
</ul>
</li>
<li><p>可以写个shell脚本一键启动&#x2F;关闭集群服务，如果使用 ZK，那么在启动 kafka 之前一定要先启动 ZK：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">! /bin/bash</span></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">	for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">  	do</span><br><span class="line">  		echo &quot;启动 $i kafka&quot;</span><br><span class="line">        ssh $i &quot;/opt/kafka/bin/kafka-server-start.sh -daemon /opt/kafka/config/server.properties&quot;</span><br><span class="line">  	done</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">	for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">  	do</span><br><span class="line">        echo &quot;停止 $i kafka&quot;</span><br><span class="line">        ssh $i &quot;/opt/kafka/bin/kafka-server-stop.sh&quot;</span><br><span class="line">  	done</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure></li>
</ul>
<ul>
<li>查看kafka是否启动成功：<code>jps</code>，一个显示当前所有java进程pid的命令</li>
</ul>
<blockquote>
<p><strong>注意：</strong>停止 Kafka 集群时，一定要等 Kafka 所有节点进程全部停止后再停止 Zookeeper集群。因为 Zookeeper 集群当中记录着 Kafka 集群相关信息，Zookeeper 集群一旦先停止，Kafka 集群就没有办法再获取停止进程的信息，只能手动杀死 Kafka 进程了。</p>
</blockquote>
<h2 id="命令行操作"><a href="#命令行操作" class="headerlink" title="命令行操作"></a>命令行操作</h2><p> <strong>主题命令行操作</strong></p>
<p>查看操作主题命令参数：<code>[root@nanzx kafka]# bin/kafka-topics.sh</code></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>–bootstrap-server &lt;String: server toconnect to&gt;</td>
<td>连接的 Kafka Broker 主机名称和端口号。</td>
</tr>
<tr>
<td>–topic &lt;String: topic&gt;</td>
<td>操作的 topic 名称。</td>
</tr>
<tr>
<td>–create</td>
<td>创建主题。</td>
</tr>
<tr>
<td>–delete</td>
<td>删除主题。</td>
</tr>
<tr>
<td>–alter</td>
<td>修改主题。</td>
</tr>
<tr>
<td>–list</td>
<td>查看所有主题。</td>
</tr>
<tr>
<td>–describe</td>
<td>查看主题详细描述。</td>
</tr>
<tr>
<td>–partitions &lt;Integer: # of partitions&gt;</td>
<td>设置分区数。</td>
</tr>
<tr>
<td>–replication-factor&lt;Integer: replication factor&gt;</td>
<td>设置分区副本。</td>
</tr>
<tr>
<td>–config &lt;String: name&#x3D;value&gt;</td>
<td>更新系统默认的配置。</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">创建主题</span> </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">--partitions 定义分区数</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">--replication-factor 定义副本数</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">--topic 定义 topic 名</span></span><br><span class="line">[root@nanzx kafka]# bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --partitions 1 --replication-factor 1 --topic firstTopic</span><br><span class="line">Created topic firstTopic.</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">查看所有主题</span></span><br><span class="line">[root@nanzx kafka]# bin/kafka-topics.sh --bootstrap-server localhost:9092 --list</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">查看主题的详情</span></span><br><span class="line">[root@nanzx kafka]# bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic firstTopic</span><br><span class="line">Topic: firstTopic	TopicId: tuPooPIlRq6nLJw4Yi-XkQ	PartitionCount: 1	ReplicationFactor: 1	Configs: segment.bytes=1073741824</span><br><span class="line">	Topic: firstTopic	Partition: 0	Leader: 0	Replicas: 0	Isr: 0</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">修改分区数，分区数只能增加，不能减少</span></span><br><span class="line">[root@nanzx kafka]# bin/kafka-topics.sh --bootstrap-server localhost:9092 --alter --topic firstTopic  --partitions 3</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">再次查看主题的详情</span></span><br><span class="line">[root@nanzx kafka]# bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic firstTopic</span><br><span class="line">Topic: firstTopic	TopicId: tuPooPIlRq6nLJw4Yi-XkQ	PartitionCount: 3	ReplicationFactor: 1	Configs: segment.bytes=1073741824</span><br><span class="line">	Topic: firstTopic	Partition: 0	Leader: 0	Replicas: 0	Isr: 0</span><br><span class="line">	Topic: firstTopic	Partition: 1	Leader: 0	Replicas: 0	Isr: 0</span><br><span class="line">	Topic: firstTopic	Partition: 2	Leader: 0	Replicas: 0	Isr: 0</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">删除主题</span>	</span><br><span class="line">[root@nanzx kafka]# bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic firstTopic</span><br></pre></td></tr></table></figure>

<hr>
<p><strong>生产者命令行操作</strong></p>
<p>查看操作生产者命令参数：<code>bin/kafka-console-producer.sh</code></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>–bootstrap-server &lt;String: server toconnect to&gt;</td>
<td>连接的 Kafka Broker 主机名称和端口号。</td>
</tr>
<tr>
<td>–topic &lt;String: topic&gt;</td>
<td>操作的 topic 名称。</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">发送消息</span></span><br><span class="line">[root@nanzx kafka]# bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic firstTopic</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">hello nan</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">nanzx.top</span></span><br></pre></td></tr></table></figure>

<hr>
<p><strong>消费者命令行操作</strong></p>
<p>查看操作消费者命令参数：<code>bin/kafka-console-consumer.sh</code></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>–bootstrap-server &lt;String: server toconnect to&gt;</td>
<td>连接的 Kafka Broker 主机名称和端口号。</td>
</tr>
<tr>
<td>–topic &lt;String: topic&gt;</td>
<td>操作的 topic 名称。</td>
</tr>
<tr>
<td>–from-beginning</td>
<td>从头开始消费。</td>
</tr>
<tr>
<td>–group &lt;String: consumer group id&gt;</td>
<td>指定消费者组名称。</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">消费主题中的数据, --from-beginning可以把主题中所有的数据都读取出来（包括历史数据）</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">[root@nanzx kafka]# bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic firstTopic</span></span><br><span class="line">hello nan</span><br><span class="line">nanzx.top</span><br></pre></td></tr></table></figure>

<h1 id="Kafka-生产者"><a href="#Kafka-生产者" class="headerlink" title="Kafka 生产者"></a>Kafka 生产者</h1><h2 id="消息发送流程"><a href="#消息发送流程" class="headerlink" title="消息发送流程"></a>消息发送流程</h2><p>在消息发送的过程中，涉及到了两个线程：<strong>main 线程和 sender 线程</strong>。在 main 线程中创建了一个<strong>双端队列 RecordAccumulator</strong>（默认 32 m，由内存池分配空间）。main 线程将消息发送给 RecordAccumulator，sender 线程不断从 RecordAccumulator 中拉取消息发送到 Kafka Broker。</p>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220226192238.png"></p>
<p><strong>main线程：</strong></p>
<ol>
<li><p>在 main 线程中创建 Producer 对象，Producer 调用 send 方法，经过拦截器（可选项，建议不用），拦截器可以对数据进行加工包装（例：Flume 拦截器）</p>
</li>
<li><p>随后经过序列化器，对数据进行传输前的<a target="_blank" rel="noopener" href="https://blog.csdn.net/tree_ifconfig/article/details/82766587?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164587532316780271917661%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=164587532316780271917661&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-4-82766587.pc_search_result_cache&utm_term=%E5%BA%8F%E5%88%97%E5%8C%96&spm=1018.2226.3001.4187">序列化</a>操作，转换为可存储和传输的字节序列。不使用Java序列化器是因为会携带大量辅助和保证安全传输的数据，在大数据场景下不适用。</p>
</li>
<li><p>接着是分区器，分区器会判断要将数据发送到哪一个分区，并将数据（每16K一批）放到不同分区对应的DQuene（一个分区创建一个DQuene，方便数据的管理）中</p>
</li>
</ol>
<p><strong>sender 线程：</strong></p>
<ol>
<li>当RecordAccumulator中的数据积累到<code>batch.size</code>（默认16k）或者数据迟迟未达到 <code>batch.size</code>，而Sender 已经等待 <code>linger.time</code> （默认是0ms，表示没有延迟）之后 Sender 线程就会从缓存双端队列中拉取并发送数据。</li>
<li>Sender 线程将拉取的数据封装成请求并放到 Broker 对应的发送队列中，发送到 Kafka 集群的某个节点，默认每个 broker 节点最多缓存5个请求。</li>
<li>由 Selector 进行发送操作，kafka 集群收到数据之后就会同步到对应的<strong>副本</strong>当中，同时也会提供 ACK 应答机制：<ol>
<li>0表示生产者发送过来的数据，不需要等数据落盘应答。</li>
<li>1表示生产者发送过来的数据，Leader收到数据后应答。</li>
<li>-1或all表示生产者发送过来的数据，Leader和ISR队列里面的所有节点（可以理解成所有副本）收齐数据后应答。</li>
</ol>
</li>
<li>Selector 反馈是成功时，清理 Broker 对应的发送队列中的请求，同时清理双端队列对应的分区数据，反馈是失败时，则会重新发送请求，默认次数是Int类型的最大值（无限，可以修改）</li>
</ol>
<hr>
<h2 id="生产者重要参数列表"><a href="#生产者重要参数列表" class="headerlink" title="生产者重要参数列表"></a>生产者重要参数列表</h2><table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>bootstrap.servers</td>
<td>生产者连接集群所需的broker地址清 单 。例如hadoop102:9092,hadoop103:9092,hadoop104:9092，可以中间用逗号隔开。注意这里并非需要所有的 broker 地址，因为生产者可以从给定的 broker里查找到其他 broker 信息。</td>
</tr>
<tr>
<td>key.serializer 和 value.serializer</td>
<td>指定发送消息的 key 和 value 的序列化类型。一定要写全类名。</td>
</tr>
<tr>
<td>buffer.memory</td>
<td>RecordAccumulator 缓冲区总大小，<strong>默认32m</strong>。</td>
</tr>
<tr>
<td>batch.size</td>
<td>缓冲区一批数据最大值，<strong>默认16k</strong>。适当增加该值，可以提高吞吐量，但是如果该值设置太大，会导致数据传输延迟增加。</td>
</tr>
<tr>
<td>linger.ms</td>
<td>如果数据迟迟未达到 batch.size，sender 等待 linger.time之后就会发送数据。单位ms，<strong>默认值是 0ms</strong>，表示没有延迟。生产环境建议该值大小为 5-100ms 之间。</td>
</tr>
<tr>
<td>acks</td>
<td>0：生产者发送过来的数据，不需要等数据落盘应答。<br/>1：生产者发送过来的数据，Leader 收到数据后应答。<br/>-1（all）：生产者发送过来的数据，Leader+和 isr 队列里面的所有节点收齐数据后应答。<br/><strong>默认值是-1</strong>，-1 和all 是等价的。</td>
</tr>
<tr>
<td>max.in.flight.requests.per.connection</td>
<td>允许最多没有返回 ack 的次数，<strong>默认为 5</strong>，开启幂等性要保证该值是 1-5 的数字。</td>
</tr>
<tr>
<td>retries</td>
<td>当消息发送出现错误的时候，系统会重发消息。retries表示重试次数。<strong>默认是 int 最大值，</strong>2147483647。<br/>如果设置了重试，还想保证消息的有序性，需要设置MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION&#x3D;1<br/>否则在重试此失败消息的时候，其他的消息可能发送成功了。</td>
</tr>
<tr>
<td>retry.backoff.ms</td>
<td>两次重试之间的时间间隔，默认是 100ms。</td>
</tr>
<tr>
<td>enable.idempotence</td>
<td>是否开启幂等性，<strong>默认 true，开启幂等性</strong>。</td>
</tr>
<tr>
<td>compression.type</td>
<td>生产者发送的所有数据的压缩方式。<strong>默认是 none，也就是不压缩</strong>。   支持压缩类型：none、gzip、snappy、lz4 和 zstd。</td>
</tr>
</tbody></table>
<h2 id="异步发送API"><a href="#异步发送API" class="headerlink" title="异步发送API"></a>异步发送API</h2><p><strong>普通异步发送：</strong></p>
<p>异步发送指的是外部数据发送到双端队列 RecordAccumulator，不需要等待前一批数据成功发送到kafka后才发送。</p>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220226192238.png"></p>
<p>引入kafka相关依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>编写不带回调函数的 API 代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">CustomProducer</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="comment">// 1. 创建 kafka 生产者的配置对象</span></span><br><span class="line">    <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">    properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.2.110:9092&quot;</span>);</span><br><span class="line">    <span class="comment">// key,value 序列化（必须）</span></span><br><span class="line">    properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">    properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 创建 kafka 生产者对象</span></span><br><span class="line">    KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 调用 send 方法,发送消息</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">        producer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;firstTopic&quot;</span>, <span class="string">&quot;Test&quot;</span> + i));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5. 关闭资源</span></span><br><span class="line">    producer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>测试：在 IDEA 中执行代码，观察虚拟机控制台中是否接收到消息。</p>
<hr>
<p><strong>带回调函数的异步发送：</strong></p>
<p>回调函数会在 producer 收到 ack （双端队列发送的）时调用，为异步调用，该方法有两个参数，分别是元数据信息（RecordMetadata）和异常信息（Exception），如果 Exception 为 null，说明消息发送成功，如果 Exception 不为 null，说明消息发送失败。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line">   <span class="keyword">void</span> <span class="title function_">CustomProducerCallback</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line">       <span class="comment">// 1. 创建 kafka 生产者的配置对象</span></span><br><span class="line">       <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">       properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.2.110:9092&quot;</span>);</span><br><span class="line">       <span class="comment">// key,value 序列化（必须）</span></span><br><span class="line">       properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">       properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 3. 创建 kafka 生产者对象</span></span><br><span class="line">       KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 4. 调用 send 方法,发送消息</span></span><br><span class="line">       <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">           producer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;firstTopic&quot;</span>, <span class="string">&quot;Test&quot;</span> + i), <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123;</span><br><span class="line">               <span class="meta">@Override</span></span><br><span class="line">               <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata recordMetadata, Exception exception)</span> &#123;</span><br><span class="line">                   <span class="keyword">if</span> (exception == <span class="literal">null</span>) &#123;</span><br><span class="line">                       <span class="comment">// 没有异常,输出信息到控制台</span></span><br><span class="line">                       System.out.println(<span class="string">&quot; 主题： &quot;</span> + recordMetadata.topic() + <span class="string">&quot;-&gt;&quot;</span> + <span class="string">&quot;分区：&quot;</span> + recordMetadata.partition());</span><br><span class="line">                   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                       <span class="comment">// 出现异常打印</span></span><br><span class="line">                       exception.printStackTrace();</span><br><span class="line">                   &#125;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;);</span><br><span class="line"></span><br><span class="line">           <span class="comment">// 延迟一会会看到数据发往不同分区</span></span><br><span class="line">           Thread.sleep(<span class="number">2</span>);</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 5. 关闭资源</span></span><br><span class="line">       producer.close();</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>

<h2 id="同步发送API"><a href="#同步发送API" class="headerlink" title="同步发送API"></a>同步发送API</h2><p>同步发送指的是外部数据发送到双端队列 RecordAccumulator，需要等待前一批数据成功发送到kafka后才发送。</p>
<p>只需在异步发送的基础上，再调用一下 get()方法即可。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">CustomProducerSync</span><span class="params">()</span> <span class="keyword">throws</span> ExecutionException, InterruptedException &#123;</span><br><span class="line">    <span class="comment">// 1. 创建 kafka 生产者的配置对象</span></span><br><span class="line">    <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">    properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.2.110:9092&quot;</span>);</span><br><span class="line">    <span class="comment">// key,value 序列化（必须）</span></span><br><span class="line">    properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">    properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 创建 kafka 生产者对象</span></span><br><span class="line">    KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 调用 send 方法,发送消息</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">        producer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;firstTopic&quot;</span>, <span class="string">&quot;Test&quot;</span> + i)).get();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5. 关闭资源</span></span><br><span class="line">    producer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="分区策略"><a href="#分区策略" class="headerlink" title="分区策略"></a>分区策略</h2><p>Kafka分区的好处：</p>
<ul>
<li><p><strong>便于合理使用存储资源</strong>，每个Partition在一个Broker上存储，可以把海量的数据按照分区切割成一块一块数据存储在多台Broker上。合理控制分区的任务，可以实现负载均衡的效果。 </p>
</li>
<li><p><strong>提高并行度</strong>，生产者可以以分区为单位发送数据；消费者可以以分区为单位进行消费数据，不同分区之间互不干扰。</p>
</li>
</ul>
<hr>
<p>KafkaProducer的默认的分区器是 <strong>DefaultPartitioner</strong>：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The default partitioning strategy:</span></span><br><span class="line"><span class="comment"> * &lt;ul&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;If a partition is specified in the record, use it</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;If no partition is specified but a key is present choose a partition based on a hash of the key</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;If no partition or key is present choose the sticky partition that changes when the batch is full.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * See KIP-480 for details about sticky partitioning.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">DefaultPartitioner</span> <span class="keyword">implements</span> <span class="title class_">Partitioner</span> &#123;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>默认的分区策略是：</p>
<ul>
<li>指明partition的情况下，<strong>直接将指明的值作为partition值</strong>；例如partition&#x3D;0，所有数据写入分区0</li>
<li>没有指明partition值但有key的情况下，将key的hash值与topic的partition数进行<strong>取余</strong>得到partition值；<ul>
<li>例如：key1的hash值&#x3D;5， key2的hash值&#x3D;6 ，topic的partition数&#x3D;2，那 么key1 对应的value1写入1号分区，key2对应的value2写入0号分区。</li>
</ul>
</li>
<li>既没有partition值又没有key值的情况下，Kafka采用<strong>Sticky Partition</strong>（黏性分区器），会<strong>随机</strong>选择一个分区，并尽可能一直使用该分区，待该分区的batch已满或者已完成，Kafka再随机一个分区进行使用（和上一次的分区<strong>不同</strong>）。<ul>
<li>例如：第一次随机选择0号分区，等0号分区当前批次满了（默认16k）或者linger.ms设置的时间到， Kafka再随机一个分区进行使用（如果还是0会继续随机）。</li>
</ul>
</li>
</ul>
<p>查看send方法的参数<strong>ProducerRecord的构造方法</strong>：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//指明partition的情况下，直接将指明的值作为partition值；例如partition=0，所有数据写入分区0</span></span><br><span class="line"><span class="keyword">public</span> <span class="title function_">ProducerRecord</span><span class="params">(String topic, Integer partition, Long timestamp, K key, V value, Iterable&lt;Header&gt; headers)</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">public</span> <span class="title function_">ProducerRecord</span><span class="params">(String topic, Integer partition, Long timestamp, K key, V value)</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">public</span> <span class="title function_">ProducerRecord</span><span class="params">(String topic, Integer partition, K key, V value, Iterable&lt;Header&gt; headers)</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">public</span> <span class="title function_">ProducerRecord</span><span class="params">(String topic, Integer partition, K key, V value)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//没有指明partition值但有key的情况下，将key的hash值与topic的partition数进行取余得到partition值</span></span><br><span class="line"><span class="keyword">public</span> <span class="title function_">ProducerRecord</span><span class="params">(String topic, K key, V value)</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">//采用Sticky Partition（黏性分区器），会随机选择一个分区</span></span><br><span class="line"><span class="keyword">public</span> <span class="title function_">ProducerRecord</span><span class="params">(String topic, V value)</span></span><br></pre></td></tr></table></figure>

<hr>
<p><strong>自定义分区器：</strong></p>
<ol>
<li><p>定义类实现 Partitioner 接口。 </p>
</li>
<li><p>重写 partition()方法。</p>
</li>
<li><p>使用分区器的方法，在生产者的配置中添加分区器参数。<code>properties.setProperty(ProducerConfig.PARTITIONER_CLASS_CONFIG,&quot;top.nanzx.kafka.producer.MyPartitioner&quot;);</code></p>
</li>
</ol>
<h2 id="生产经验"><a href="#生产经验" class="headerlink" title="生产经验"></a>生产经验</h2><h3 id="提高吞吐量"><a href="#提高吞吐量" class="headerlink" title="提高吞吐量"></a>提高吞吐量</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//根据需求选择合适参数：        </span></span><br><span class="line">        <span class="comment">// batch.size：批次大小，默认16K</span></span><br><span class="line">        properties.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">16384</span>);</span><br><span class="line">        <span class="comment">// linger.ms：等待时间，默认0ms</span></span><br><span class="line">        properties.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">1</span>);</span><br><span class="line">        <span class="comment">// buffer.memory：RecordAccumulator缓冲区大小，默认 32M</span></span><br><span class="line">        properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG,<span class="number">33554432</span>);</span><br><span class="line">        <span class="comment">// compression.type：压缩，默认 none，可配置值 gzip、snappy、lz4 和 zstd</span></span><br><span class="line">        properties.put(ProducerConfig.COMPRESSION_TYPE_CONFIG,<span class="string">&quot;snappy&quot;</span>);</span><br></pre></td></tr></table></figure>

<h3 id="数据可靠性（ACK应答机制）"><a href="#数据可靠性（ACK应答机制）" class="headerlink" title="数据可靠性（ACK应答机制）"></a>数据可靠性（ACK应答机制）</h3><blockquote>
<p>ACK应答级别：</p>
<ul>
<li><p>0：生产者发送过来的数据，不需要等数据落盘应答。<strong>可靠性差，效率高；</strong></p>
</li>
<li><p>1：生产者发送过来的数据，Leader收到数据后应答。<strong>可靠性中等，效率中等；</strong></p>
</li>
<li><p>-1（all）：生产者发送过来的数据，Leader和ISR队列里面的所有节点收齐数据后应答。<strong>可靠性高，效率低；</strong></p>
</li>
</ul>
</blockquote>
<p><strong>acks&#x3D;0</strong>，生产者发送过来数据就不管了：</p>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220301223217.png"></p>
<hr>
<p><strong>acks&#x3D;1</strong>，生产者发送过来数据Leader应答：</p>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220301223231.png"></p>
<hr>
<p><strong>acks&#x3D;-1（all）</strong>，生产者发送过来数据Leader和ISR队列里面所有Follwer应答：</p>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220301223931.png"></p>
<p><strong>思考</strong>：Leader收到数据，所有Follower都开始同步数据，但有一个Follower，因为某种故障，迟迟不能与Leader进行同步，那这个问题怎么解决呢？</p>
<p>Leader维护了一个动态的<strong>in-sync replica set（ISR）</strong>：和Leader<strong>保持同步</strong>的Follower+Leader集合(leader:0,isr:0,1,2)。</p>
<p>如果Follower长时间未向Leader发送通信请求或同步数据，则该Follower将被踢出ISR。该时间阈值由<code>replica.lag.time.max.ms</code>参数设定，默认30s。例如2超时，(leader:0, isr:0,1)。</p>
<p>这样就不用等长期联系不上或者已经故障的节点。</p>
<p><strong>总结：</strong>在生产环境中，acks&#x3D;0 很少使用；acks&#x3D;1 一般用于传输普通日志，允许丢个别数据；acks&#x3D;-1，一般用于传输和钱相关的数据，对可靠性要求比较高的场景。</p>
<p><strong>数据可靠性分析：</strong></p>
<p>如果分区副本设置为1个，或者ISR里应答的最小副本数量（ min.insync.replicas 默认为1）设置为1，和ack&#x3D;1的效果是一样的，仍然有丢数的风险（leader：0，isr：0）。</p>
<blockquote>
<p><strong>数据完全可靠条件 &#x3D; ACK级别设置为-1 + 分区副本大于等于2 + ISR里应答的最小副本数量大于等于2</strong></p>
</blockquote>
<hr>
<h3 id="数据去重（幂等性和事务）"><a href="#数据去重（幂等性和事务）" class="headerlink" title="数据去重（幂等性和事务）"></a>数据去重（幂等性和事务）</h3><p><strong>数据重复分析：</strong></p>
<p>acks： -1（all）：生产者发送过来的数据，Leader和ISR队列里面的所有节点收齐数据后应答。</p>
<p>Leader和ISR队列里面的所有节点收齐数据后Leader准备应答时挂了，Producer没有收到ack应答，于是重新给新Leader节点发送数据造成数据重复。</p>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220302201453.png"></p>
<p><strong>数据传递语义：</strong></p>
<p>• 至少一次（At Least Once）&#x3D; ACK级别设置为-1 + 分区副本大于等于2 + ISR里应答的最小副本数量大于等于2 （数据完全可靠条件），可以保证数据不丢失，但是不能保证数据不重复</p>
<p>• 最多一次（At Most Once）&#x3D; ACK级别设置为0 ，可以保证数据不重复，但是不能保证数据不丢失。</p>
<p>• 精确一次（Exactly Once）：对于一些非常重要的信息，比如和钱相关的数据，要求数据既不能重复也不丢失。</p>
<blockquote>
<p>Kafka 0.11版本以后，引入了一项重大特性：<strong>幂等性</strong>和<strong>事务</strong>。</p>
</blockquote>
<hr>
<p><strong>幂等性</strong></p>
<p>幂等性是指Producer不论向Broker发送多少次重复数据，Broker端都只会持久化一条，保证了不重复。</p>
<p>精确一次（Exactly Once） &#x3D; 幂等性 + 至少一次（ ack&#x3D;-1 + 分区副本数&gt;&#x3D;2 + ISR最小副本数量&gt;&#x3D;2） 。 </p>
<p><strong>重复数据的判断标准</strong>：具有<code>&lt;PID, Partition, SeqNumber&gt;</code>相同主键的消息提交时，Broker只会持久化一条。其中PID是Kafka每次重启都会分配一个新的；Partition 表示分区号；Sequence Number是单调自增的。</p>
<p>所以幂等性只能保证的是在<strong>单分区单会话</strong>内不重复。</p>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220302204338.png"></p>
<p><strong>如何使用幂等性：</strong>开启参数 <strong>enable.idempotence</strong> 默认为 true，false 关闭。</p>
<hr>
<p><strong>事务</strong></p>
<p>幂等性只能保证单分区单会话，远远不够，跨分区跨会话仍会导致数据重复问题，因此引入事务。</p>
<p><strong>说明：开启事务，必须开启幂等性。</strong></p>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220302220620.png"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">CustomProducerTransaction</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="comment">// 1. 创建 kafka 生产者的配置对象</span></span><br><span class="line">    <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">    properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.2.110:9092&quot;</span>);</span><br><span class="line">    <span class="comment">// key,value 序列化（必须）</span></span><br><span class="line">    properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">    properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置事务 id（必须），事务 id 任意起名</span></span><br><span class="line">    properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, <span class="string">&quot;transaction_id_0&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 创建 kafka 生产者对象</span></span><br><span class="line">    KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化事务</span></span><br><span class="line">    producer.initTransactions();</span><br><span class="line">    <span class="comment">// 开启事务</span></span><br><span class="line">    producer.beginTransaction();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 4. 调用 send 方法,发送消息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            producer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;firstTopic&quot;</span>, <span class="string">&quot;Test&quot;</span> + i));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 提交事务</span></span><br><span class="line">        producer.commitTransaction();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        <span class="comment">// 终止事务</span></span><br><span class="line">        producer.abortTransaction();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="comment">// 5. 关闭资源</span></span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="数据有序"><a href="#数据有序" class="headerlink" title="数据有序"></a>数据有序</h3><p><strong>单分区内有序</strong>，但多分区时，分区与分区间无序是无序的。</p>
<p>为了保证多分区时数据有序，有两种解决方案：</p>
<ul>
<li>生产者端统一采用<strong>一个分区</strong>发送请求</li>
<li>消费者端对请求重排序，保证数据的有序处理，但是效率低下，需要等待请求全部送达</li>
</ul>
<p>kafka在1.x版本之前保证数据单分区有序，条件如下：</p>
<ul>
<li><code>max.in.flight.requests.per.connection</code>&#x3D;1（不需要考虑是否开启幂等性，因为只有一个请求）。</li>
</ul>
<p>kafka在1.x及以后版本保证数据单分区有序，条件如下：</p>
<ul>
<li><p>开启幂等性：<code>max.in.flight.requests.per.connection</code>需要设置小于等于5。 </p>
</li>
<li><p>未开启幂等性：<code>max.in.flight.requests.per.connection</code>需要设置为1。</p>
</li>
</ul>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220302221914.png"></p>
<p>原因说明：在kafka1.x以后，启用幂等后，kafka服务端会缓存producer发来的最近5个request的元数据并进行排序，故无论如何，都可以保证最近5个request的数据都是有序的。</p>
<hr>
<h1 id="Kafka-Broker"><a href="#Kafka-Broker" class="headerlink" title="Kafka Broker"></a>Kafka Broker</h1><h2 id="Broker-工作流程"><a href="#Broker-工作流程" class="headerlink" title="Broker 工作流程"></a>Broker 工作流程</h2><p>启动 Zookeeper 客户端，使用 ls 命令在kafka目录下（配置文件中连接ZK时的设置），可以查看 Zookeeper 存储的 Kafka 相关信息：</p>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220302233820.png"></p>
<ul>
<li>Zookeeper 中会记录整个集群中那些 broker 可用&#x2F;上线【&#x2F;brokers&#x2F;ids[0,1,2]】，0 1 2表示broker的id，在kafka配置文件指定</li>
<li>也会记录每一个 partition 中的 leader 信息以及动态维护的<strong>in-sync replica set（ISR）</strong>（和Leader<strong>保持同步</strong>的Follower+Leader集合）【&#x2F;brokers&#x2F;topics&#x2F;主题名称&#x2F;partition&#x2F;第几个分区&#x2F;state】</li>
</ul>
<hr>
<p> <strong>Broker总体工作流程：</strong></p>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220302234808.png"></p>
<ol>
<li>每台 broker 启动后在 ZK 节点中注册</li>
<li>每个broker都有各自的Controller模块，抢占 ZK 节点的controller ，谁先注册谁就是Controller Leader</li>
<li>选举出来的 Controller 监听 brokers 节点变化</li>
<li>Controller 决定副本 Leader 的选举，以<strong>在 ISR 中存活为前提</strong>，按照 AR （Assigned Replica，Kafka分区中所有副本的统称）中<strong>排在最前面</strong>的优先。</li>
<li>将Leader 副本信息和 ISR 信息上传到 ZK 中存储</li>
<li>其他 Controller节点从 ZK 中同步消息，防止Controller Leader挂了，随时准备成为Leader</li>
<li>Producer向Leader发送信息，Follower主动向Leader同步信息。同步信息在底层以1G的Segmen的log文件配合index文件的方式进行存储。</li>
<li>如果 Leader 所在的 broker 挂了，Controller Leader监听到 ZK 注册的节点发生了变化，会获取 ISR 再重新选举Leader 副本并更新 ZK 中的Leader信息以及 ISR（ISR为前提，<strong>AR为先后顺序</strong>）</li>
</ol>
<h2 id="Broker-重要参数列表"><a href="#Broker-重要参数列表" class="headerlink" title="Broker 重要参数列表"></a>Broker 重要参数列表</h2><table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>replica.lag.time.max.ms</td>
<td>ISR 中，如果 Follower 长时间未向 Leader 发送通信请求或同步数据，则该 Follower 将被踢出 ISR。该时间阈值默认是 <strong>30s</strong>。</td>
</tr>
<tr>
<td>auto.leader.rebalance.enable</td>
<td>默认是 true。 自动 Leader Partition 平衡。</td>
</tr>
<tr>
<td>leader.imbalance.per.broker.percentage</td>
<td>**默认是 10%**。每个 broker 允许的不平衡的 leader的比率。如果每个 broker 超过了这个值，控制器会触发 leader 的平衡。</td>
</tr>
<tr>
<td>leader.imbalance.check.interval.seconds</td>
<td><strong>默认值 300 秒</strong>。检查 leader 负载是否平衡的间隔时间。</td>
</tr>
<tr>
<td>log.segment.bytes</td>
<td>Kafka 中 log 日志是分成一块块存储的，此配置是指 log 日志划分成块的大小，<strong>默认值 1G</strong>。</td>
</tr>
<tr>
<td>log.index.interval.bytes</td>
<td><strong>默认 4kb</strong>，kafka 里面每当写入了 4kb 大小的日志（.log），然后就往 index 文件里面记录一个索引。</td>
</tr>
<tr>
<td>log.retention.hours</td>
<td>Kafka 中数据保存的时间，<strong>默认 7 天</strong>。</td>
</tr>
<tr>
<td>log.retention.minutes</td>
<td>Kafka 中数据保存的时间，<strong>分钟级别</strong>，默认关闭。</td>
</tr>
<tr>
<td>log.retention.ms</td>
<td>Kafka 中数据保存的时间，<strong>毫秒级别</strong>，默认关闭。</td>
</tr>
<tr>
<td>log.retention.check.interval.ms</td>
<td>检查数据是否保存超时的间隔，<strong>默认是 5 分钟</strong>。</td>
</tr>
<tr>
<td>log.retention.bytes</td>
<td><strong>默认等于-1，表示无穷大</strong>。超过设置的所有日志总大小，删除最早的 segment。</td>
</tr>
<tr>
<td>log.cleanup.policy</td>
<td><strong>默认是 delete</strong>，表示所有数据启用删除策略；<br/>如果设置值为 compact，表示所有数据启用压缩策略。</td>
</tr>
<tr>
<td>num.io.threads</td>
<td>负责写磁盘的线程数，<strong>默认是 8</strong>。这个参数值占总核数的 50%</td>
</tr>
<tr>
<td>num.replica.fetchers</td>
<td>副本拉取线程数，这个参数占总核数的 50%的 1&#x2F;3</td>
</tr>
<tr>
<td>num.network.threads</td>
<td>数据传输线程数，<strong>默认是 3</strong>。这个参数占总核数的50%的 2&#x2F;3 。</td>
</tr>
<tr>
<td>log.flush.interval.messages</td>
<td>强制页缓存刷写到磁盘的条数，默认是 long 的最大值，9223372036854775807。一般不建议修改，交给系统自己管理。</td>
</tr>
<tr>
<td>log.flush.interval.ms</td>
<td>每隔多久刷数据到磁盘，默认是 null。一般不建议修改，交给系统自己管理。</td>
</tr>
</tbody></table>
<h2 id="Kafka-副本"><a href="#Kafka-副本" class="headerlink" title="Kafka 副本"></a>Kafka 副本</h2><ul>
<li><p>Kafka 副本作用：提高数据可靠性。 </p>
</li>
<li><p>Kafka 默认副本 1 个，生产环境一般配置为 2 个，保证数据可靠性；太多副本会增加磁盘存储空间，增加网络上数据传输，降低效率。</p>
</li>
<li><p>Kafka 中副本分为：Leader 和 Follower。Kafka 生产者只会把数据发往 Leader，然后 Follower 找 Leader 进行同步数据。</p>
</li>
<li><p>Kafka 分区中的所有副本统称为 AR（Assigned Repllicas）。</p>
<ul>
<li><p>AR &#x3D; ISR + OSR</p>
</li>
<li><p><strong>ISR</strong>，表示和 Leader 保持同步的 Follower 集合。如果 Follower 长时间未向 Leader 发送通信请求或同步数据，则该 Follower 将被踢出 ISR。该时间阈值由 <strong>replica.lag.time.max.ms</strong>参数设定，默认 30s。Leader 发生故障之后，就会从 ISR 中选举新的 Leader。</p>
</li>
<li><p><strong>OSR</strong>，表示 Follower 与 Leader 副本同步时，延迟过多的副本。</p>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Leader-和-Follower-故障处理细节"><a href="#Leader-和-Follower-故障处理细节" class="headerlink" title="Leader 和 Follower 故障处理细节"></a>Leader 和 Follower 故障处理细节</h3><ul>
<li><p>Offset：消息在对应 Topic 中的偏移量</p>
</li>
<li><p>LEO（Log End Offset）：每个副本的最后一个offset，LEO其实就是最新的offset + 1。</p>
</li>
<li><p>HW（High Watermark）：所有副本中最小的LEO 。</p>
</li>
</ul>
<p><strong>Follower故障：</strong></p>
<p>（1） Follower发生故障后会被临时踢出ISR</p>
<p>（2） 这个期间Leader和Follower继续接收数据</p>
<p>（3）待该Follower恢复后，Follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向Leader进行同步。</p>
<p>（4）等该Follower的LEO大于等于该Partition的HW，即Follower追上Leader之后，就可以重新加入ISR了。</p>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220304233728.png"></p>
<p><strong>Leader故障：</strong></p>
<p>（1） Leader发生故障之后，会从ISR中选出一个新的Leader</p>
<p>（2）为保证多个副本之间的数据一致性，其余的Follower会先将各自的log文件高于HW的部分截掉，然后从新的Leader同步数据。</p>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220304233744.png"></p>
<p><strong>注意：</strong>这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。</p>
<hr>
<h3 id="分区副本分配"><a href="#分区副本分配" class="headerlink" title="分区副本分配"></a>分区副本分配</h3><p>如果 kafka 服务器只有 4 个节点，那么设置 kafka 的分区数大于服务器台数，在 kafka底层如何分配存储副本呢？</p>
<p>创建一个新的 topic，16 分区，3 个副本：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[nanzx kafka]$ bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --partitions 16 --replication-factor 3 --topic second</span><br><span class="line"></span><br><span class="line">[nanzx kafka]$ bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic second</span><br><span class="line">Topic: second4 Partition: 0 Leader: 0 Replicas: 0,1,2 Isr: 0,1,2</span><br><span class="line">Topic: second4 Partition: 1 Leader: 1 Replicas: 1,2,3 Isr: 1,2,3</span><br><span class="line">Topic: second4 Partition: 2 Leader: 2 Replicas: 2,3,0 Isr: 2,3,0</span><br><span class="line">Topic: second4 Partition: 3 Leader: 3 Replicas: 3,0,1 Isr: 3,0,1</span><br><span class="line">Topic: second4 Partition: 4 Leader: 0 Replicas: 0,2,3 Isr: 0,2,3</span><br><span class="line">Topic: second4 Partition: 5 Leader: 1 Replicas: 1,3,0 Isr: 1,3,0</span><br><span class="line">Topic: second4 Partition: 6 Leader: 2 Replicas: 2,0,1 Isr: 2,0,1</span><br><span class="line">Topic: second4 Partition: 7 Leader: 3 Replicas: 3,1,2 Isr: 3,1,2</span><br><span class="line">Topic: second4 Partition: 8 Leader: 0 Replicas: 0,3,1 Isr: 0,3,1</span><br><span class="line">Topic: second4 Partition: 9 Leader: 1 Replicas: 1,0,2 Isr: 1,0,2</span><br><span class="line">Topic: second4 Partition: 10 Leader: 2 Replicas: 2,1,3 Isr: 2,1,3</span><br><span class="line">Topic: second4 Partition: 11 Leader: 3 Replicas: 3,2,0 Isr: 3,2,0</span><br><span class="line">Topic: second4 Partition: 12 Leader: 0 Replicas: 0,1,2 Isr: 0,1,2</span><br><span class="line">Topic: second4 Partition: 13 Leader: 1 Replicas: 1,2,3 Isr: 1,2,3</span><br><span class="line">Topic: second4 Partition: 14 Leader: 2 Replicas: 2,3,0 Isr: 2,3,0</span><br><span class="line">Topic: second4 Partition: 15 Leader: 3 Replicas: 3,0,1 Isr: 3,0,1</span><br></pre></td></tr></table></figure>

<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220305161930.png"></p>
<hr>
<h3 id="手动调整分区副本存储"><a href="#手动调整分区副本存储" class="headerlink" title="手动调整分区副本存储"></a>手动调整分区副本存储</h3><ul>
<li><p>当有节点服役和退役时，副本应如何重新分配（使用生成的副本存储计划）；</p>
</li>
<li><p>在生产环境中，每台服务器的配置和性能不一致，但是Kafka只会根据自己的代码规则创建对应的分区副本，就会导致个别服务器存储压力较大。所以需要手动调整分区副本的存储（自己指定副本存储计划）。</p>
</li>
<li><p>在生产环境当中，由于某个主题的重要等级需要提升，我们考虑增加副本。副本数的增加需要先制定计划，然后根据计划执行（自己指定副本存储计划）。</p>
</li>
</ul>
<p>创建一个主题需要均衡的json文件：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[nanzx kafka]$ vim topics-to-move.json</span><br><span class="line">&#123;</span><br><span class="line"> &quot;topics&quot;: [</span><br><span class="line"> 	&#123;&quot;topic&quot;: &quot;first&quot;&#125;</span><br><span class="line"> ],</span><br><span class="line"> &quot;version&quot;: 1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>生成一个负载均衡的计划（–broker-list是选择将副本分配到哪些broker上）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[nanzx kafka]$ bin/kafka-reassign-partitions.sh --bootstrap-server 127.0.0.1:9092 --topics-to-move-json-file topics-to-move.json --broker-list &quot;0,1,2,3&quot; --generate</span><br><span class="line">Current partition replica assignment</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0,2,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,1,0],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,0,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;</span><br><span class="line">Proposed partition reassignment configuration</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[2,3,0],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[3,0,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[0,1,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;</span><br></pre></td></tr></table></figure>

<p>创建副本存储计划（使用生成的计划将所有副本存储在 broker0、broker1、broker2、broker3 中）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[nanzx kafka]$ vim increase-replication-factor.json</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[2,3,0],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[3,0,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[0,1,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;</span><br></pre></td></tr></table></figure>

<p>执行副本存储计划</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[nanzx kafka]$ bin/kafka-reassign-partitions.sh --bootstrap-server 127.0.0.1:9092 --reassignment-json-file increase-replication-factor.json --execute</span><br></pre></td></tr></table></figure>

<p>验证副本存储计划</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[nanzx kafka]$ bin/kafka-reassign-partitions.sh --bootstrap-server 127.0.0.1:9092 --reassignment-json-file increase-replication-factor.json --verify</span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition first-0 is complete.</span><br><span class="line">Reassignment of partition first-1 is complete.</span><br><span class="line">Reassignment of partition first-2 is complete.</span><br><span class="line">Clearing broker-level throttles on brokers 0,1,2,3</span><br><span class="line">Clearing topic-level throttles on topic first</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="Leader-Partition自动平衡"><a href="#Leader-Partition自动平衡" class="headerlink" title="Leader Partition自动平衡"></a>Leader Partition自动平衡</h3><p>正常情况下，Kafka本身会自动把Leader Partition均匀分散在各个机器上，来保证每台机器的读写吞吐量都是均匀的。但是如果某些broker宕机，会导致Leader Partition过于集中在其他少部分几台broker上，这会导致少数几台broker的读写请求压力过高，其他宕机的broker重启之后都是follower partition，读写请求很低，造成集群负载不均衡。</p>
<blockquote>
<p>• auto.leader.rebalance.enable，默认是true。自动 Leader Partition 平衡。</p>
<p>• leader.imbalance.per.broker.percentage，默认是10%。每个broker允许的不平衡的leader的比率。如果每个broker超过了这个值，控制器会触发leader的平衡。</p>
<p>• leader.imbalance.check.interval.seconds，默认值300秒。检查leader负载是否平衡的间隔时间。</p>
</blockquote>
<p>下面拿一个主题举例说明，假设集群只有一个主题如下图所示：</p>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220305205620.png"></p>
<p>分区2的<strong>AR优先副本是0节点</strong>（第三行，replicas可以理解成AR），但是0节点却不是Leader节点，所以broker0节点的不平衡数加1，AR副本总数是4，所以broker0节点不平衡率为1&#x2F;4&gt;10%，需要再平衡。</p>
<p>broker2和broker3节点和broker0不平衡率一样，需要再平衡。Broker1的不平衡数为0，不需要再平衡。</p>
<h2 id="文件存储机制"><a href="#文件存储机制" class="headerlink" title="文件存储机制"></a>文件存储机制</h2><p>Topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是Producer生产的数据。Producer生产的数据会被不断<strong>追加</strong>到该log文件末端，为防止log文件过大导致数据定位效率低，Kafka采取了<strong>分片</strong>和<strong>索引</strong>机制， 将<strong>每个partition分为多个segment</strong>。每个segment包括：“.index”文件、“.log”文件和.timeindex等文件。这些文件位于一个文件夹下，文件夹的命名规则为：topic名称+分区序号，例如：first-0。</p>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220306001210.png"></p>
<p>说明：index文件和log文件都是以当前segment的第一条消息的offset命名。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">查看的/opt/kafka/datas/first-1 （first-0、first-2）路径上的文件。</span></span><br><span class="line">[nanzx first-1]$ ls</span><br><span class="line">00000000000000000092.index</span><br><span class="line">00000000000000000092.log</span><br><span class="line">00000000000000000092.snapshot</span><br><span class="line">00000000000000000092.timeindex</span><br><span class="line">leader-epoch-checkpoint</span><br><span class="line">partition.metadata</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="built_in">cat</span>查看会中文乱码</span></span><br><span class="line">[nanzx first-1]$ cat 00000000000000000092.log </span><br><span class="line">\CYnF|©|©ÿÿÿÿÿÿÿÿÿÿÿÿÿÿ&quot;hello world</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">使用kafka的工具查看index文件</span></span><br><span class="line">[nanzx first-1]$ kafka-run-class.sh kafka.tools.DumpLogSegments --files./00000000000000000000.index</span><br><span class="line">Dumping ./00000000000000000000.index</span><br><span class="line">offset: 3 position: 152</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">使用kafka的工具查看<span class="built_in">log</span>文件</span></span><br><span class="line">[nanzx first-1]$ kafka-run-class.sh kafka.tools.DumpLogSegments </span><br><span class="line">--files ./00000000000000000000.log</span><br><span class="line">Dumping datas/first-0/00000000000000000000.log</span><br><span class="line">Starting offset: 0</span><br><span class="line">baseOffset: 0 lastOffset: 1 count: 2 baseSequence: -1 lastSequence: -1 producerId: -1 </span><br><span class="line">producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: </span><br><span class="line">0 CreateTime: 1636338440962 size: 75 magic: 2 compresscodec: none crc: 2745337109 isvalid: </span><br><span class="line">true</span><br><span class="line">baseOffset: 2 lastOffset: 2 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 </span><br><span class="line">producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: </span><br><span class="line">75 CreateTime: 1636351749089 size: 77 magic: 2 compresscodec: none crc: 273943004 isvalid: </span><br><span class="line">true</span><br><span class="line">baseOffset: 3 lastOffset: 3 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 </span><br><span class="line">producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: </span><br><span class="line">152 CreateTime: 1636351749119 size: 77 magic: 2 compresscodec: none crc: 106207379 isvalid: </span><br><span class="line">true</span><br><span class="line">baseOffset: 4 lastOffset: 8 count: 5 baseSequence: -1 lastSequence: -1 producerId: -1 </span><br><span class="line">producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: </span><br><span class="line">229 CreateTime: 1636353061435 size: 141 magic: 2 compresscodec: none crc: 157376877 isvalid: </span><br><span class="line">true</span><br><span class="line">baseOffset: 9 lastOffset: 13 count: 5 baseSequence: -1 lastSequence: -1 producerId: -1 </span><br><span class="line">producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: </span><br><span class="line">370 CreateTime: 1636353204051 size: 146 magic: 2 compresscodec: none crc: 4058582827 isvalid: </span><br><span class="line">true</span><br></pre></td></tr></table></figure>

<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220306234956.png"></p>
<p>说明：日志存储参数配置</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>log.segment.bytes</td>
<td>Kafka 中 log 日志是分成一块块存储的，此配置是指 log 日志划分成块的大小，<strong>默认值 1G</strong>。</td>
</tr>
<tr>
<td>log.index.interval.bytes</td>
<td><strong>默认 4kb</strong>，kafka 里面每当写入了 4kb 大小的日志（.log），然后就往 index 文件里面记录一个索引。 稀疏索引。</td>
</tr>
</tbody></table>
<h2 id="文件清理策略"><a href="#文件清理策略" class="headerlink" title="文件清理策略"></a>文件清理策略</h2><p>Kafka 中<strong>默认的日志保存时间为 7 天</strong>，可以通过调整如下参数修改保存时间。</p>
<ul>
<li><p>log.retention.hours，最低优先级小时，默认 7 天。 </p>
</li>
<li><p>log.retention.minutes，分钟。 </p>
</li>
<li><p>log.retention.ms，最高优先级毫秒。 </p>
</li>
<li><p>log.retention.check.interval.ms，负责设置检查周期，默认 5 分钟。</p>
</li>
</ul>
<p>那么日志一旦超过了设置的时间，怎么处理呢？Kafka 中提供的日志清理策略有 delete 和 compact 两种。 </p>
<p><strong>delete 日志删除</strong>：将过期数据删除</p>
<p>log.cleanup.policy &#x3D; delete，所有数据启用删除策略</p>
<ul>
<li><p>基于时间：默认打开。以 segment 中所有记录中的<strong>最大时间戳</strong>作为该文件时间戳。如果一个 segment 中有一部分数据过期，一部分没有过期，则保留。</p>
</li>
<li><p>基于大小：默认关闭。超过设置的所有日志总大小，删除最早的 segment。log.retention.bytes，默认等于-1，表示无穷大。</p>
</li>
</ul>
<p><strong>compact日志压缩</strong>：对于相同key的不同value值，只保留最后一个版本。</p>
<p> log.cleanup.policy &#x3D; compact 所有数据启用压缩策略</p>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220307235402.png"></p>
<p>压缩后的offset可能是不连续的，比如上图中没有6，当从这些offset消费消息时，将会拿到比这个offset大的offset对应的消息，实际上会拿到offset为7的消息，并从这个位置开始消费。</p>
<p>这种策略只适合特殊场景，比如消息的key是用户ID，value是用户的资料，通过这种压缩策略，整个消息集里就保存了所有用户最新的资料。 </p>
<hr>
<h2 id="高效读写数据"><a href="#高效读写数据" class="headerlink" title="高效读写数据"></a>高效读写数据</h2><ul>
<li><p>Kafka 本身是<strong>分布式集群</strong>，可以采用<strong>分区</strong>技术，并行度高</p>
</li>
<li><p>读数据采用<strong>稀疏索引</strong>，可以快速定位要消费的数据</p>
</li>
<li><p>顺序写磁盘</p>
<ul>
<li>Kafka 的 producer 生产数据，要写入到 log 文件中，写的过程是一直<strong>追加</strong>到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到 600M&#x2F;s，而随机写只有 100K&#x2F;s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。</li>
</ul>
</li>
</ul>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220306235743.png"></p>
<ul>
<li><p>页缓存 + 零拷贝技术</p>
<ul>
<li><p><strong>零拷贝：</strong>Kafka的数据加工处理操作交由Kafka生产者和Kafka消费者处理。Kafka Broker应用层不关心存储的数据，所以就<strong>不用走应用层</strong>，传输效率高。</p>
</li>
<li><p><strong>PageCache页缓存</strong>：Kafka重度依赖底层操作系统提供的PageCache功 能。当上层有写操作时，操作系统只是将数据写入PageCache。当读操作发生时，先从PageCache中查找，如果找不到，再去磁盘中读取。实际上PageCache是把尽可能多的空闲内存都当做了磁盘缓存来使用。</p>
</li>
</ul>
</li>
</ul>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220307000053.png"></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>log.flush.interval.messages</td>
<td>强制页缓存刷写到磁盘的条数，默认是 long 的最大值，9223372036854775807。一般不建议修改，交给系统自己管理。</td>
</tr>
<tr>
<td>log.flush.interval.ms</td>
<td>每隔多久，刷数据到磁盘，默认是 null。一般不建议修改，交给系统自己管理。</td>
</tr>
</tbody></table>
<h1 id="Kafka-消费者"><a href="#Kafka-消费者" class="headerlink" title="Kafka 消费者"></a>Kafka 消费者</h1><h2 id="Kafka-消费方式"><a href="#Kafka-消费方式" class="headerlink" title="Kafka 消费方式"></a>Kafka 消费方式</h2><ul>
<li>Push 模式：<strong>队列推送消息给消费者</strong></li>
</ul>
<ol>
<li><ul>
<li>缺陷：由broker决定消息发送速率，很难适应所有消费者的消费速率，导致 Consumer 来不及处理消息</li>
</ul>
</li>
</ol>
<ul>
<li>Pull 模式：Kafka采用这种方式。<strong>消费者主动到队列中拉取</strong></li>
</ul>
<ol>
<li><ul>
<li>缺陷：如果队列中没有数据，消费者可能会陷入循环中，一直返回空数据</li>
</ul>
</li>
<li><ul>
<li>改进：设定一个 timeout 参数并在消费者消费数据时由队列传给消费者，表示消费者在没有消息处理时等待一段时间后再来拉取</li>
<li>Pull 模式提高消费吞吐量：<ul>
<li>增加 Topic 分区数，并同时增加消费者数量</li>
<li>提高每批次拉取的数据量，避免消费者资源浪费</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="消费者组"><a href="#消费者组" class="headerlink" title="消费者组"></a>消费者组</h2><p>Consumer Group：消费者组，由多个consumer组成。形成一个消费者组的条件是所有消费者的<strong>groupid相同</strong>。 </p>
<ul>
<li><p>消费者组内每个消费者负责消费不同分区的数据，<strong>一个分区只能由组内一个消费者消费</strong>。 </p>
</li>
<li><p><strong>消费者组之间互不影响</strong>。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</p>
</li>
<li><p>如果向消费组中添加更多的消费者，超过主题分区数量，则有一部分消费者就会<strong>闲置</strong>，不会接收任何消息。</p>
</li>
</ul>
<hr>
<h3 id="消费者组初始化流程"><a href="#消费者组初始化流程" class="headerlink" title="消费者组初始化流程"></a>消费者组初始化流程</h3><p><strong>coordinator</strong>：辅助实现消费者组的初始化和分区的分配。每个消费者的offset由消费者提交到coordinator中保存。</p>
<p>coordinator节点选择 &#x3D; groupid的hashcode值 % 50（ __consumer_offsets的分区数量）</p>
<p>例如： groupid的hashcode值 &#x3D; 1，1% 50 &#x3D; 1，那么__consumer_offsets 主题的1号分区在哪个broker上，就选择这个节点的coordinator作为这个消费者组的老大。消费者组下的所有的消费者提交offset的时候就往这个分区去提交offset。 </p>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220307235316.png"></p>
<h3 id="消费者组详细消费流程"><a href="#消费者组详细消费流程" class="headerlink" title="消费者组详细消费流程"></a>消费者组详细消费流程</h3><p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220308000040.png"></p>
<h2 id="消费者重要参数列表"><a href="#消费者重要参数列表" class="headerlink" title="消费者重要参数列表"></a>消费者重要参数列表</h2><table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>bootstrap.servers</td>
<td>向 Kafka 集群建立初始连接用到的 host&#x2F;port 列表。</td>
</tr>
<tr>
<td>key.deserializer 和 value.deserializer</td>
<td>指定接收消息的 key 和 value 的反序列化类型。一定要写全类名。</td>
</tr>
<tr>
<td>group.id</td>
<td>标记消费者所属的消费者组。</td>
</tr>
<tr>
<td>enable.auto.commit</td>
<td><strong>默认值为 true</strong>，消费者会自动周期性地向服务器提交偏移量。</td>
</tr>
<tr>
<td>auto.commit.interval.ms</td>
<td>如果设置了 enable.auto.commit 的值为 true， 则该值定义了消费者偏移量向 Kafka 提交的频率，<strong>默认 5s</strong>。</td>
</tr>
<tr>
<td>auto.offset.reset</td>
<td>当 Kafka 中没有初始偏移量或当前偏移量在服务器中不存在<br/>（如，数据被删除了），该如何处理？ <br/>earliest：自动重置偏移量到最早的偏移量。 <br/> <strong>latest：默认</strong>，自动重置偏移量为最新的偏移量。 <br/> none：如果消费组原来的（previous）偏移量不存在，则向消费者抛异常。anything：向消费者抛异常。</td>
</tr>
<tr>
<td>offsets.topic.num.partitions</td>
<td>__consumer_offsets 的分区数，<strong>默认是 50 个分区</strong>。</td>
</tr>
<tr>
<td>heartbeat.interval.ms</td>
<td>Kafka 消费者和 coordinator 之间的心跳时间，<strong>默认 3s</strong>。<br/>该条目的值必须小于 session.timeout.ms ，也不应该高于session.timeout.ms 的 1&#x2F;3。</td>
</tr>
<tr>
<td>session.timeout.ms</td>
<td>Kafka 消费者和 coordinator 之间连接超时时间，<strong>默认 45s</strong>。<br/>超过该值，该消费者被移除，消费者组执行再平衡。</td>
</tr>
<tr>
<td>max.poll.interval.ms</td>
<td>消费者处理消息的最大时长，<strong>默认是 5 分钟</strong>。<br/>超过该值，该消费者被移除，消费者组执行再平衡。</td>
</tr>
<tr>
<td>fetch.min.bytes</td>
<td><strong>默认 1 个字节</strong>。消费者获取服务器端一批消息最小的字节数。</td>
</tr>
<tr>
<td>fetch.max.wait.ms</td>
<td><strong>默认 500ms</strong>。如果没有从服务器端获取到一批数据的最小字节数。该时间到，仍然会返回数据。</td>
</tr>
<tr>
<td>fetch.max.bytes</td>
<td>默认 Default: 52428800（<strong>50 m</strong>）。消费者获取服务器端一批消息最大的字节数。如果服务器端一批次的数据大于该值（50m）仍然可以拉取回来这批数据，因此，这不是一个绝对最大值。<br/>一批次的大小受 message.max.bytes （broker config）or max.message.bytes （topic config）影响。</td>
</tr>
<tr>
<td>max.poll.records</td>
<td>一次 poll 拉取数据返回消息的最大条数，<strong>默认是 500 条</strong>。</td>
</tr>
</tbody></table>
<h2 id="消费者API"><a href="#消费者API" class="headerlink" title="消费者API"></a>消费者API</h2><p><strong>注意：</strong>在消费者 API 代码中必须配置消费者组 id。命令行启动消费者不填写消费者组id会被自动填写随机的消费者组 id。 </p>
<p><strong>订阅主题：</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"> <span class="keyword">void</span> <span class="title function_">CustomCustomer</span><span class="params">()</span> &#123;</span><br><span class="line">     <span class="comment">// 1.创建消费者的配置对象</span></span><br><span class="line">     <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">     <span class="comment">// 2.给消费者配置对象添加参数</span></span><br><span class="line">     properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.2.110:9092&quot;</span>);</span><br><span class="line">     <span class="comment">// 配置序列化 必须</span></span><br><span class="line">     properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">     properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">     <span class="comment">// 配置消费者组（组名任意起名） 必须</span></span><br><span class="line">     properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line"></span><br><span class="line">     <span class="comment">// 创建消费者对象</span></span><br><span class="line">     KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(properties);</span><br><span class="line">     <span class="comment">// 注册要消费的主题（可以消费多个主题）</span></span><br><span class="line">     ArrayList&lt;String&gt; topics = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">     topics.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">     kafkaConsumer.subscribe(topics);</span><br><span class="line">     <span class="comment">// 拉取数据打印</span></span><br><span class="line">     <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">         <span class="comment">// 设置 1s 中消费一批数据</span></span><br><span class="line">         ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">         <span class="comment">// 打印消费到的数据</span></span><br><span class="line">         <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">             System.out.println(consumerRecord);</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p><strong>订阅分区：</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">CustomConsumerPartition</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">    properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.2.110:9092&quot;</span>);</span><br><span class="line">    <span class="comment">// 配置序列化 必须</span></span><br><span class="line">    properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">    properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">    <span class="comment">// 配置消费者组（必须），名字可以任意起</span></span><br><span class="line">    properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">    </span><br><span class="line">    KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(properties);</span><br><span class="line">    <span class="comment">// 消费某个主题的某个分区数据</span></span><br><span class="line">    ArrayList&lt;TopicPartition&gt; topicPartitions = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">    topicPartitions.add(<span class="keyword">new</span> <span class="title class_">TopicPartition</span>(<span class="string">&quot;first&quot;</span>, <span class="number">0</span>));</span><br><span class="line">    kafkaConsumer.assign(topicPartitions);</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; consumerRecords =</span><br><span class="line">                kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord :</span><br><span class="line">                consumerRecords) &#123;</span><br><span class="line">            System.out.println(consumerRecord);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="分区的分配以及再平衡"><a href="#分区的分配以及再平衡" class="headerlink" title="分区的分配以及再平衡"></a>分区的分配以及再平衡</h2><p>一个消费者组中有多个consumer组成，一个 topic有多个partition组成，现在的问题是，到底由哪个consumer来消费哪个partition的数据。 </p>
<p>Kafka有四种主流的分区分配策略： Range、RoundRobin、Sticky、CooperativeSticky。可以通过配置参数<code>partition.assignment.strategy</code>，修改分区的分配策略。默认策略是<strong>Range + CooperativeSticky</strong>。Kafka可以同时使用多个分区分配策略。</p>
<p>当以下事件发生时，Kafka 将会进行一次<strong>分区分配（也称为 Rebalance 重平衡）</strong>：</p>
<ul>
<li>同一个 Consumer Group 内新增消费者（组内消费者数量变化）</li>
<li>消费者离开当前所属的 Consumer Group，包括 shuts down 或 crashes</li>
<li>订阅的主题新增分区</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 修改分区分配策略</span></span><br><span class="line">properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, <span class="string">&quot;org.apache.kafka.clients.consumer.RoundRobinAssignor&quot;</span>);</span><br></pre></td></tr></table></figure>

<h3 id="Range-以及再平衡"><a href="#Range-以及再平衡" class="headerlink" title="Range 以及再平衡"></a>Range 以及再平衡</h3><p><strong>Range 分区策略原理</strong>：</p>
<ul>
<li><p>Range 是对<strong>每个 topic</strong> 而言的。</p>
</li>
<li><p>首先对同一个 topic 里面的<strong>分区按照序号进行排序</strong>，并对<strong>消费者按照字母顺序进行排序</strong>。</p>
<ul>
<li>假如现在有 7 个分区，3 个消费者，排序后的分区将会是0,1,2,3,4,5,6；消费者排序完之后将会是C0,C1,C2。</li>
</ul>
</li>
<li><p>通过 partitions数&#x2F;consumer数来决定每个消费者应该消费几个分区。如果除不尽，那么前面几个消费者将会多消费 1 个分区。</p>
<ul>
<li>例如，7&#x2F;3 &#x3D; 2 余 1 ，除不尽，那么 消费者 C0 便会多消费 1 个分区。 8&#x2F;3&#x3D;2余2，除不尽，那么C0和C1分别多消费一个。</li>
</ul>
</li>
</ul>
<p>注意：如果只是针对 1 个 topic 而言，C0消费者多消费1个分区影响不是很大。但是如果有 N 多个 topic，那么针对每个 topic，消费者 C0都将多消费 1 个分区，topic越多，C0消 费的分区会比其他消费者明显多消费 N 个分区。 容易产生<strong>数据倾斜</strong>！</p>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220313235556.png"></p>
<p><strong>Range 分区分配再平衡案例：</strong></p>
<p>（1）停止掉 0 号消费者，快速重新发送消息观看结果（45s 以内，越快越好）。 </p>
<ul>
<li><p>1 号消费者：消费到 3、4 号分区数据。 </p>
</li>
<li><p>2 号消费者：消费到 5、6 号分区数据。 </p>
</li>
<li><p>0 号消费者的任务会整体被分配到 1 号消费者或者 2 号消费者。</p>
</li>
</ul>
<p>说明：0 号消费者挂掉后，消费者组需要按照超时时间 45s 来判断它是否退出，所以需要等待时间到了 45s 后，判断它真的退出就会把任务分配给其他 broker 执行。</p>
<p>（2）再次重新发送消息观看结果（45s 以后）。 </p>
<ul>
<li><p>1 号消费者：消费到 0、1、2、3 号分区数据。 </p>
</li>
<li><p>2 号消费者：消费到 4、5、6 号分区数据。</p>
</li>
</ul>
<p>说明：消费者 0 已经被踢出消费者组，所以重新按照 range 方式分配。</p>
<hr>
<h3 id="RoundRobin-以及再平衡"><a href="#RoundRobin-以及再平衡" class="headerlink" title="RoundRobin 以及再平衡"></a>RoundRobin 以及再平衡</h3><p><strong>RoundRobin 分区策略原理：</strong></p>
<p>RoundRobin 针对集群中<strong>所有Topic</strong>而言。</p>
<p>RoundRobin 轮询分区策略，是把所有的 partition 和所有的consumer 都列出来，然后<strong>按照 hashcode 进行排序</strong>，最后通过<strong>轮询算法</strong>来分配 partition 给到各个消费者。</p>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220313235342.png"></p>
<p><strong>RoundRobin</strong> <strong>分区分配再平衡案例</strong></p>
<p>（1）停止掉 0 号消费者，快速重新发送消息观看结果（45s 以内，越快越好）。 </p>
<ul>
<li><p>1 号消费者：消费到 1、4 号分区数据</p>
</li>
<li><p>2 号消费者：消费到 2、5 号分区数据</p>
</li>
<li><p>0 号消费者的任务会按照 RoundRobin 的方式，把数据轮询分成 0 、3 和 6 号分区数据，分别由 1 号消费者或者 2 号消费者消费。</p>
</li>
</ul>
<p>说明：0 号消费者挂掉后，消费者组需要按照超时时间 45s 来判断它是否退出，所以需要等待，时间到了 45s 后，判断它真的退出就会把任务分配给其他 broker 执行。</p>
<p>（2）再次重新发送消息观看结果（45s 以后）。 </p>
<ul>
<li><p>1 号消费者：消费到 0、2、4、6 号分区数据</p>
</li>
<li><p>2 号消费者：消费到 1、3、5 号分区数据</p>
</li>
</ul>
<p>说明：消费者 0 已经被踢出消费者组，所以重新按照 RoundRobin 方式分配。</p>
<hr>
<h3 id="Sticky-以及再平衡"><a href="#Sticky-以及再平衡" class="headerlink" title="Sticky 以及再平衡"></a>Sticky 以及再平衡</h3><p><strong>粘性分区定义：</strong>可以理解为分配的结果带有“粘性的”。即在执行一次新的分配之前，考虑上一次分配的结果，尽量少的调整分配的变动，可以节省大量的开销。</p>
<p>粘性分区是 Kafka 从 0.11.x 版本开始引入这种分配策略，首先会尽量均衡的放置分区到消费者上面，在同一消费者组内消费者出现问题的时候，会尽量保持原有分配的分区不变化。</p>
<p><strong>Sticky</strong> <strong>分区分配再平衡案例</strong></p>
<p>（1）停止掉 0 号消费者，快速重新发送消息观看结果（45s 以内，越快越好）。 </p>
<ul>
<li><p>1 号消费者：消费到 2、5、3 号分区数据。 </p>
</li>
<li><p>2 号消费者：消费到 4、6 号分区数据。 </p>
</li>
<li><p>0 号消费者的任务会按照粘性规则，尽可能均衡的随机分成 0 和 1 号分区数据，分别由 1 号消费者或者 2 号消费者消费。</p>
</li>
</ul>
<p>说明：0 号消费者挂掉后，消费者组需要按照超时时间 45s 来判断它是否退出，所以需要等待，时间到了 45s 后，判断它真的退出就会把任务分配给其他 broker 执行。</p>
<p>（2）再次重新发送消息观看结果（45s 以后）。 </p>
<ul>
<li><p>1 号消费者：消费到 2、3、5 号分区数据。 </p>
</li>
<li><p>2 号消费者：消费到 0、1、4、6 号分区数据。</p>
</li>
</ul>
<p>说明：消费者 0 已经被踢出消费者组，所以重新按照粘性方式分配。</p>
<hr>
<h2 id="offset位移"><a href="#offset位移" class="headerlink" title="offset位移"></a>offset位移</h2><p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220314235740.png"></p>
<p>__consumer_offsets 主题里面采用 key 和 value 的方式存储数据。<strong>key 是 group.id+topic+分区号</strong>，value 就是当前 offset 的值。每隔一段时间，kafka 内部会对这个 topic 进行compact，也就是每个group.id+topic+分区号就保留最新数据。</p>
<p><strong>消费 offset 案例：</strong></p>
<p>在配置文件 config&#x2F;consumer.properties 中添加配置 <strong>exclude.internal.topics&#x3D;false</strong>，默认是 true，表示不能消费系统主题。为了查看该系统主题数据，所以该参数修改为 false。</p>
<p>采用命令行方式，创建一个新的 topic：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[nanzx kafka]$ bin/kafka-topics.sh --bootstrap-server 192.168.2.110:9092 --create --topic mytopic --partitions 2 --replication-factor 2</span><br></pre></td></tr></table></figure>

<p>启动生产者往 mytopic 生产数据：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[nanzx kafka]$ bin/kafka-console-producer.sh --topic mytopic --bootstrap-server 192.168.2.110:9092</span><br></pre></td></tr></table></figure>

<p>启动消费者消费 mytopic 数据：</p>
<blockquote>
<p>注意：指定消费者组名称，更好观察数据存储位置（key 是 group.id+topic+分区号）。 </p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[nanzx kafka]$ bin/kafka-console-consumer.sh --bootstrap-server 192.168.2.110:9092 --topic atguigu --group test</span><br></pre></td></tr></table></figure>

<p>查看消费者消费主题__consumer_offsets：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[nanzx kafka]$ bin/kafka-console-consumer.sh --topic __consumer_offsets --bootstrap-server 192.168.2.110:9092 --consumer.config config/consumer.properties --formatter </span><br><span class="line">&quot;kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter&quot; --from-beginning</span><br><span class="line">...</span><br><span class="line">[test,mytopic,1]::OffsetAndMetadata(offset=7,leaderEpoch=Optional[0], metadata=,commitTimestamp=1622442520203, expireTimestamp=None)</span><br><span class="line">[test,mytopic,0]::OffsetAndMetadata(offset=8, leaderEpoch=Optional[0], metadata=,commitTimestamp=1622442520203, expireTimestamp=None)</span><br></pre></td></tr></table></figure>

<h3 id="自动提交-offset"><a href="#自动提交-offset" class="headerlink" title="自动提交 offset"></a>自动提交 offset</h3><p>为了使我们能够专注于自己的业务逻辑，Kafka提供了自动提交offset的功能。</p>
<p>自动提交offset的相关参数：</p>
<ul>
<li><p>enable.auto.commit：是否开启自动提交offset功能，<strong>默认是true</strong>，消费者会周期性自动地向服务器提交偏移量。</p>
</li>
<li><p>auto.commit.interval.ms：自动提交offset的时间间隔，默认是5s</p>
</li>
</ul>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220315230324.png"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 是否自动提交 offset</span></span><br><span class="line">properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="literal">true</span>);</span><br><span class="line"><span class="comment">// 提交 offset 的时间周期 1000ms，默认是5s</span></span><br><span class="line">properties.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, <span class="number">1000</span>);</span><br></pre></td></tr></table></figure>

<h3 id="手动提交-offset"><a href="#手动提交-offset" class="headerlink" title="手动提交 offset"></a>手动提交 offset</h3><p>虽然自动提交offset十分简单便利，但由于其是基于时间提交的，开发人员难以把握offset提交的时机。因此Kafka还提供了手动提交offset的API。</p>
<p>手动提交offset的方法有两种：</p>
<ul>
<li><p>commitSync（同步提交）：必须等待offset提交完毕，再去消费下一批数据。</p>
</li>
<li><p>commitAsync（异步提交） ：发送完提交offset请求后，就开始消费下一批数据了。</p>
</li>
</ul>
<p>两者的相同点是，都会将本次提交的一批数据最高的偏移量提交；不同点是，同步提交阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而异步提交则没有失败重试机制，故有可能提交失败。 </p>
<p><strong>同步提交 offset：</strong>由于同步提交 offset 有失败重试机制，故更加可靠，但是由于一直等待提交结果，提交的效率比较低。以下为同步提交 offset 的示例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">CustomConsumerByHandSync</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">    properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line">    <span class="comment">// 配置序列化 必须</span></span><br><span class="line">    properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">    properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">    <span class="comment">// 配置消费者组</span></span><br><span class="line">    properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">    <span class="comment">// 是否自动提交 offset</span></span><br><span class="line">    properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="literal">false</span>);</span><br><span class="line">    <span class="comment">// 创建 kafka 消费者</span></span><br><span class="line">    KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(properties);</span><br><span class="line">    <span class="comment">// 设置消费主题 形参是列表</span></span><br><span class="line">    kafkaConsumer.subscribe(Collections.singletonList(<span class="string">&quot;first&quot;</span>));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="comment">// 读取消息</span></span><br><span class="line">        ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">            System.out.println(consumerRecord);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 同步提交 offset</span></span><br><span class="line">        kafkaConsumer.commitSync();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>异步提交 offset：</strong>虽然同步提交 offset 更可靠一些，但是由于其会阻塞当前线程，直到提交成功。因此吞吐量会受到很大的影响。因此<strong>更多的情况下，会选用异步提交</strong> offset 的方式。以下为异步提交 offset 的示例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">CustomConsumerByHandAsync</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">    properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line">    <span class="comment">// 配置序列化 必须</span></span><br><span class="line">    properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">    properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">    <span class="comment">// 配置消费者组</span></span><br><span class="line">    properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">    <span class="comment">// 是否自动提交 offset</span></span><br><span class="line">    properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="literal">false</span>);</span><br><span class="line">    <span class="comment">// 创建 kafka 消费者</span></span><br><span class="line">    KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(properties);</span><br><span class="line">    <span class="comment">// 设置消费主题 形参是列表</span></span><br><span class="line">    kafkaConsumer.subscribe(Collections.singletonList(<span class="string">&quot;first&quot;</span>));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="comment">// 读取消息</span></span><br><span class="line">        ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">            System.out.println(consumerRecord);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 异步提交 offset</span></span><br><span class="line">        kafkaConsumer.commitAsync();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="指定-Offset-消费"><a href="#指定-Offset-消费" class="headerlink" title="指定 Offset 消费"></a>指定 Offset 消费</h3><p>auto.offset.reset &#x3D; earliest | latest | none 默认是 latest。 </p>
<p>当 Kafka 中没有初始偏移量（消费者组第一次消费）或服务器上不再存在当前偏移量时（例如该数据已被删除），该怎么办？ </p>
<p>（1）earliest：自动将偏移量重置为最早的偏移量，**–from-beginning**。 </p>
<p>（2）latest（<strong>默认值</strong>）：自动将偏移量重置为最新偏移量。</p>
<p>（3）none：如果未找到消费者组的先前偏移量，则向消费者抛出异常。</p>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220315231605.png"></p>
<p>任意指定 offset 位移开始消费（注意：每次执行完，需要修改消费者组名）：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">CustomConsumerSeek</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">    properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line">    <span class="comment">// 配置序列化 必须</span></span><br><span class="line">    properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">    properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">    <span class="comment">// 配置消费者组</span></span><br><span class="line">    properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">    <span class="comment">// 创建 kafka 消费者</span></span><br><span class="line">    KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(properties);</span><br><span class="line">    <span class="comment">// 设置消费主题 形参是列表</span></span><br><span class="line">    kafkaConsumer.subscribe(Collections.singletonList(<span class="string">&quot;first&quot;</span>));</span><br><span class="line"></span><br><span class="line">    Set&lt;TopicPartition&gt; assignment = <span class="keyword">new</span> <span class="title class_">HashSet</span>&lt;&gt;();</span><br><span class="line">    <span class="keyword">while</span> (assignment.size() == <span class="number">0</span>) &#123;</span><br><span class="line">        kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">        <span class="comment">// 获取消费者分区分配信息（有了分区分配信息才能开始消费）</span></span><br><span class="line">        assignment = kafkaConsumer.assignment();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 遍历所有分区，并指定 offset 从 1700 的位置开始消费</span></span><br><span class="line">    <span class="keyword">for</span> (TopicPartition tp : assignment) &#123;</span><br><span class="line">        kafkaConsumer.seek(tp, <span class="number">1700</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="comment">// 读取消息</span></span><br><span class="line">        ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">            System.out.println(consumerRecord);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="指定时间消费"><a href="#指定时间消费" class="headerlink" title="指定时间消费"></a>指定时间消费</h3><p>需求：在生产环境中，会遇到最近消费的几个小时数据异常，想重新按照时间消费。</p>
<p>例如要求按照时间消费前一天的数据，怎么处理？</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">CustomConsumerForTime</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">    properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line">    <span class="comment">// 配置序列化 必须</span></span><br><span class="line">    properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">    properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">    <span class="comment">// 配置消费者组</span></span><br><span class="line">    properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">    <span class="comment">// 是否自动提交 offset</span></span><br><span class="line">    properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="literal">false</span>);</span><br><span class="line">    <span class="comment">// 创建 kafka 消费者</span></span><br><span class="line">    KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(properties);</span><br><span class="line">    <span class="comment">// 设置消费主题 形参是列表</span></span><br><span class="line">    kafkaConsumer.subscribe(Collections.singletonList(<span class="string">&quot;first&quot;</span>));</span><br><span class="line"></span><br><span class="line">    Set&lt;TopicPartition&gt; assignment = <span class="keyword">new</span> <span class="title class_">HashSet</span>&lt;&gt;();</span><br><span class="line">    <span class="keyword">while</span> (assignment.size() == <span class="number">0</span>) &#123;</span><br><span class="line">        kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">        <span class="comment">// 获取消费者分区分配信息（有了分区分配信息才能开始消费）</span></span><br><span class="line">        assignment = kafkaConsumer.assignment();</span><br><span class="line">    &#125;</span><br><span class="line">    HashMap&lt;TopicPartition, Long&gt; timestampToSearch = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">    <span class="comment">// 封装集合存储，每个分区对应一天前的数据</span></span><br><span class="line">    <span class="keyword">for</span> (TopicPartition topicPartition : assignment) &#123;</span><br><span class="line">        timestampToSearch.put(topicPartition, System.currentTimeMillis() - <span class="number">1</span> * <span class="number">24</span> * <span class="number">3600</span> * <span class="number">1000</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 获取从 1 天前开始消费的每个分区的 offset</span></span><br><span class="line">    Map&lt;TopicPartition, OffsetAndTimestamp&gt; offsets = kafkaConsumer.offsetsForTimes(timestampToSearch);</span><br><span class="line">    <span class="comment">// 遍历每个分区，对每个分区设置消费时间。</span></span><br><span class="line">    <span class="keyword">for</span> (TopicPartition topicPartition : assignment) &#123;</span><br><span class="line">        <span class="type">OffsetAndTimestamp</span> <span class="variable">offsetAndTimestamp</span> <span class="operator">=</span> offsets.get(topicPartition);</span><br><span class="line">        <span class="comment">// 根据时间指定开始消费的位置</span></span><br><span class="line">        <span class="keyword">if</span> (offsetAndTimestamp != <span class="literal">null</span>) &#123;</span><br><span class="line">            kafkaConsumer.seek(topicPartition, offsetAndTimestamp.offset());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="comment">// 读取消息</span></span><br><span class="line">        ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">            System.out.println(consumerRecord);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="漏消费和重复消费"><a href="#漏消费和重复消费" class="headerlink" title="漏消费和重复消费"></a>漏消费和重复消费</h2><p><strong>重复消费：</strong>已经消费了数据，但是 offset 没提交。</p>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220316232534.png"></p>
<p><strong>漏消费：</strong>设置offset为手动提交，当offset被提交时，数据还在内存中未落盘，此时刚好消费者线程被kill掉，那么offset已经提交，但是数据未处理，导致这部分内存中的数据丢失。</p>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220316232709.png"></p>
<p>如果想完成Consumer端的精准一次性消费，那么需要Kafka消费端将消费过程和提交offset过程做<strong>原子绑定</strong>。此时我们需要将Kafka的offset保存到支持事务的自定义介质（比如MySQL）。这部分知识会在后续项目部分涉及。</p>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220316232839.png"></p>
<h2 id="数据积压"><a href="#数据积压" class="headerlink" title="数据积压"></a>数据积压</h2><ul>
<li>如果是Kafka消费能力不足，则可以考虑增加Topic的分区数，并且同时提升消费组的消费者数量，消费者数 &#x3D; 分区数。（两者缺一不可） </li>
<li>如果是下游的数据处理不及时：提高每批次拉取的数量。批次拉取数据过少（拉取数据&#x2F;处理时间 &lt; 生产速度），使处理的数据小于生产的数据，也会造成数据积压。</li>
</ul>
<p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220316233043.png"></p>
<h1 id="Kafka监控"><a href="#Kafka监控" class="headerlink" title="Kafka监控"></a>Kafka监控</h1><p>Kafka-Eagle 框架可以监控 Kafka 集群的整体运行情况，在生产环境中经常使用。</p>
<h2 id="其他环境准备"><a href="#其他环境准备" class="headerlink" title="其他环境准备"></a>其他环境准备</h2><p> <strong>MySQL 环境准备：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run -p 3306:3306 --name mysql </span><br><span class="line">-v /root/mysql/conf:/etc/mysql/conf.d </span><br><span class="line">-v /root/mysql/logs:/logs </span><br><span class="line">-v /root/mysql/data:/var/lib/mysql </span><br><span class="line">-e MYSQL_ROOT_PASSWORD=123456 </span><br><span class="line">-d mysql</span><br></pre></td></tr></table></figure>

<p><strong>Zookeeper环境准备：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run -d</span><br><span class="line">-p 2181:2181</span><br><span class="line">-v /root/zookeeper/data/:/data/</span><br><span class="line">--name=zookeeper</span><br><span class="line">--privileged zookeeper</span><br></pre></td></tr></table></figure>

<p><strong>Kafka 环境准备：</strong></p>
<p>修改&#x2F;opt&#x2F;kafka&#x2F;bin&#x2F;kafka-server-start.sh 命令中的配置（可选，如果kafka启动不了，根目录有hs_err_pid的log，大概率是内存，可将启动脚本中堆内存参数改小，或者增加服务器内存，EKAF启动同理）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">if [ &quot;x$KAFKA_HEAP_OPTS&quot; = &quot;x&quot; ]; then</span><br><span class="line"> export KAFKA_HEAP_OPTS=&quot;-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70&quot;</span><br><span class="line"> export JMX_PORT=&quot;9999&quot;</span><br><span class="line"><span class="meta prompt_"> #</span><span class="language-bash"><span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">&quot;-Xmx1G -Xms1G&quot;</span></span></span><br><span class="line"><span class="meta prompt_"> #</span><span class="language-bash"><span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">&quot;-Xmx256M -Xms128M&quot;</span> <span class="comment">#启动不了可以这样设置，efak也可以改</span></span></span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<h2 id="Kafka-Eagle-安装"><a href="#Kafka-Eagle-安装" class="headerlink" title="Kafka-Eagle 安装"></a>Kafka-Eagle 安装</h2><ul>
<li><p>官网：<a target="_blank" rel="noopener" href="https://www.kafka-eagle.org/">https://www.kafka-eagle.org/</a></p>
</li>
<li><p>上传压缩包 kafka-eagle-bin-2.1.0.tar.gz 到集群&#x2F;root&#x2F;software 目录</p>
</li>
<li><p>解压到opt目录：</p>
</li>
</ul>
<p><code>[root@nanzx kafka-eagle-bin-2.1.0]# tar -zxvf kafka-eagle-bin-2.1.0.tar.gz -C /opt/</code></p>
<ul>
<li>进入刚才解压后的目录继续解压：</li>
</ul>
<p><code>[root@nanzx kafka-eagle-bin-2.1.0]# tar -zxvf efak-web-2.1.0-bin.tar.gz -C /opt/</code></p>
<ul>
<li>修改名称</li>
</ul>
<p><code>[root@nanzx kafka-eagle-bin-2.1.0]# mv efak-web-2.1.0 efak</code></p>
<ul>
<li>修改配置文件 &#x2F;opt&#x2F;module&#x2F;efak&#x2F;conf&#x2F;system-config.properties</li>
</ul>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># multi zookeeper &amp; kafka cluster list</span></span><br><span class="line"><span class="comment"># Settings prefixed with &#x27;kafka.eagle.&#x27; will be deprecated, use &#x27;efak.&#x27; instead</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="attr">efak.zk.cluster.alias</span>=<span class="string">cluster1</span></span><br><span class="line"><span class="attr">cluster1.zk.list</span>=<span class="string">192.168.2.110:2181,192.168.2.111:2181,192.168.2.112:2181</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># zookeeper enable acl</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="attr">cluster1.zk.acl.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="attr">cluster1.zk.acl.schema</span>=<span class="string">digest</span></span><br><span class="line"><span class="attr">cluster1.zk.acl.username</span>=<span class="string">test</span></span><br><span class="line"><span class="attr">cluster1.zk.acl.password</span>=<span class="string">test123</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># broker size online list</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="attr">cluster1.efak.broker.size</span>=<span class="string">20</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># zk client thread limit</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="attr">kafka.zk.limit.size</span>=<span class="string">16</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># EFAK webui port</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="attr">efak.webui.port</span>=<span class="string">8048</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># EFAK enable distributed</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="attr">efak.distributed.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="attr">efak.cluster.mode.status</span>=<span class="string">master</span></span><br><span class="line"><span class="attr">efak.worknode.master.host</span>=<span class="string">localhost</span></span><br><span class="line"><span class="attr">efak.worknode.port</span>=<span class="string">8085</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka jmx acl and ssl authenticate</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="attr">cluster1.efak.jmx.acl</span>=<span class="string">false</span></span><br><span class="line"><span class="attr">cluster1.efak.jmx.user</span>=<span class="string">keadmin</span></span><br><span class="line"><span class="attr">cluster1.efak.jmx.password</span>=<span class="string">keadmin123</span></span><br><span class="line"><span class="attr">cluster1.efak.jmx.ssl</span>=<span class="string">false</span></span><br><span class="line"><span class="attr">cluster1.efak.jmx.truststore.location</span>=<span class="string">/data/ssl/certificates/kafka.truststore</span></span><br><span class="line"><span class="attr">cluster1.efak.jmx.truststore.password</span>=<span class="string">ke123456</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka offset storage</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="attr">cluster1.efak.offset.storage</span>=<span class="string">kafka</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka jmx uri</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="attr">cluster1.efak.jmx.uri</span>=<span class="string">service:jmx:rmi:///jndi/rmi://%s/jmxrmi</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka metrics, 15 days by default</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="attr">efak.metrics.charts</span>=<span class="string">true</span></span><br><span class="line"><span class="attr">efak.metrics.retain</span>=<span class="string">15</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka sql topic records max</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="attr">efak.sql.topic.records.max</span>=<span class="string">5000</span></span><br><span class="line"><span class="attr">efak.sql.topic.preview.records.max</span>=<span class="string">10</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># delete kafka topic token</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="attr">efak.topic.token</span>=<span class="string">keadmin</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka sasl authenticate</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="attr">cluster1.efak.sasl.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="attr">cluster1.efak.sasl.protocol</span>=<span class="string">SASL_PLAINTEXT</span></span><br><span class="line"><span class="attr">cluster1.efak.sasl.mechanism</span>=<span class="string">SCRAM-SHA-256</span></span><br><span class="line"><span class="attr">cluster1.efak.sasl.jaas.config</span>=<span class="string">org.apache.kafka.common.security.scram.ScramLoginModule required username=&quot;kafka&quot; password=&quot;kafka-eagle&quot;;</span></span><br><span class="line"><span class="attr">cluster1.efak.sasl.client.id</span>=<span class="string"></span></span><br><span class="line"><span class="attr">cluster1.efak.blacklist.topics</span>=<span class="string"></span></span><br><span class="line"><span class="attr">cluster1.efak.sasl.cgroup.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="attr">cluster1.efak.sasl.cgroup.topics</span>=<span class="string"></span></span><br><span class="line"><span class="attr">cluster2.efak.sasl.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="attr">cluster2.efak.sasl.protocol</span>=<span class="string">SASL_PLAINTEXT</span></span><br><span class="line"><span class="attr">cluster2.efak.sasl.mechanism</span>=<span class="string">PLAIN</span></span><br><span class="line"><span class="attr">cluster2.efak.sasl.jaas.config</span>=<span class="string">org.apache.kafka.common.security.plain.PlainLoginModule required username=&quot;kafka&quot; password=&quot;kafka-eagle&quot;;</span></span><br><span class="line"><span class="attr">cluster2.efak.sasl.client.id</span>=<span class="string"></span></span><br><span class="line"><span class="attr">cluster2.efak.blacklist.topics</span>=<span class="string"></span></span><br><span class="line"><span class="attr">cluster2.efak.sasl.cgroup.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="attr">cluster2.efak.sasl.cgroup.topics</span>=<span class="string"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka ssl authenticate</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="attr">cluster3.efak.ssl.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="attr">cluster3.efak.ssl.protocol</span>=<span class="string">SSL</span></span><br><span class="line"><span class="attr">cluster3.efak.ssl.truststore.location</span>=<span class="string"></span></span><br><span class="line"><span class="attr">cluster3.efak.ssl.truststore.password</span>=<span class="string"></span></span><br><span class="line"><span class="attr">cluster3.efak.ssl.keystore.location</span>=<span class="string"></span></span><br><span class="line"><span class="attr">cluster3.efak.ssl.keystore.password</span>=<span class="string"></span></span><br><span class="line"><span class="attr">cluster3.efak.ssl.key.password</span>=<span class="string"></span></span><br><span class="line"><span class="attr">cluster3.efak.ssl.endpoint.identification.algorithm</span>=<span class="string">https</span></span><br><span class="line"><span class="attr">cluster3.efak.blacklist.topics</span>=<span class="string"></span></span><br><span class="line"><span class="attr">cluster3.efak.ssl.cgroup.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="attr">cluster3.efak.ssl.cgroup.topics</span>=<span class="string"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka sqlite jdbc driver address</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment">#efak.driver=org.sqlite.JDBC</span></span><br><span class="line"><span class="comment">#efak.url=jdbc:sqlite:/hadoop/kafka-eagle/db/ke.db</span></span><br><span class="line"><span class="comment">#efak.username=root</span></span><br><span class="line"><span class="comment">#efak.password=www.kafka-eagle.org</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka mysql jdbc driver address</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="attr">efak.driver</span>=<span class="string">com.mysql.cj.jdbc.Driver</span></span><br><span class="line"><span class="attr">efak.url</span>=<span class="string">jdbc:mysql://127.0.0.1:3306/ke?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull</span></span><br><span class="line"><span class="attr">efak.username</span>=<span class="string">root</span></span><br><span class="line"><span class="attr">efak.password</span>=<span class="string">123456</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>配置环境变量<code>vi /etc/profile</code>：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kafkaEFAK</span></span><br><span class="line"><span class="attr">export</span> <span class="string">KE_HOME=/opt/efak</span></span><br><span class="line"><span class="attr">export</span> <span class="string">PATH=$PATH:$KE_HOME/bin</span></span><br></pre></td></tr></table></figure>

<p>使环境变量生效：<code>source /etc/profile</code></p>
</li>
<li><p>启动：<code>[root@nanzx efak]# bin/ke.sh start</code></p>
<ul>
<li>注意：启动之前需要先启动 ZK 以及 KAFKA</li>
</ul>
</li>
<li><p>登录页面查看监控数据：<a target="_blank" rel="noopener" href="http://192.168.2.110:8048/">http://192.168.2.110:8048/</a></p>
<ul>
<li>启动成功后控制台会打印出用户名密码和HTTP访问地址相关信息。要注意的是，<strong>启动失败</strong>也会打印出这些信息，所以得通过链接访问成功了才能判断启动没问题。</li>
</ul>
</li>
</ul>
<h1 id="Kafka-Kraft-模式"><a href="#Kafka-Kraft-模式" class="headerlink" title="Kafka-Kraft 模式"></a>Kafka-Kraft 模式</h1><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p><img src= "https://unpkg.com/nan-picture/img/load.gif" data-lazy-src="https://unpkg.com/nan-picture/blog/20220319193800.png"></p>
<p>左图为 Kafka 现有架构，元数据在 zookeeper 中，运行时动态选举 controller，由controller 进行 Kafka 集群管理。右图为 kraft 模式架构（实验性），不再依赖 zookeeper 集群，而是用三台 controller 节点代替 zookeeper，元数据保存在 controller 中，由 controller 直接进行 Kafka 集群管理。</p>
<p>这样做的好处有以下几个： </p>
<ul>
<li><p>Kafka 不再依赖外部框架，而是能够独立运行； </p>
</li>
<li><p>controller 管理集群时，不再需要从 zookeeper 中先读取数据，集群性能上升； </p>
</li>
<li><p>由于不依赖 zookeeper，集群扩展时不再受到 zookeeper 读写能力限制； </p>
</li>
<li><p>controller <strong>不再动态选举</strong>，而是由配置文件规定。这样我们可以有针对性的加强controller 节点的配置，而不是像以前一样对随机 controller 节点的高负载束手无策。</p>
</li>
</ul>
<h2 id="安装部署-1"><a href="#安装部署-1" class="headerlink" title="安装部署"></a>安装部署</h2><p>安装Kafka，以前是进入config目录修改配置文件 server.properties，现在是修改&#x2F;config&#x2F;<strong>kraft</strong>&#x2F;server.properties 配置文件：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#kafka的角色（controller相当于主机、broker节点相当于从机，主机类似 zk 功 能）</span></span><br><span class="line"><span class="attr">process.roles</span>=<span class="string">broker, controller</span></span><br><span class="line"><span class="comment">#节点ID，不能重复</span></span><br><span class="line"><span class="attr">node.id</span>=<span class="string">2</span></span><br><span class="line"><span class="comment">#controller 服务协议别名</span></span><br><span class="line"><span class="attr">controller.listener.names</span>=<span class="string">CONTROLLER</span></span><br><span class="line"><span class="comment">#全 Controller 列表，前面需要加node.id</span></span><br><span class="line"><span class="attr">controller.quorum.voters</span>=<span class="string">2@hadoop102:9093,3@hadoop103:9093,4@hadoop104:9093</span></span><br><span class="line"><span class="comment">#不同服务器绑定的端口</span></span><br><span class="line"><span class="attr">listeners</span>=<span class="string">PLAINTEXT://:9092,CONTROLLER://:9093</span></span><br><span class="line"><span class="comment">#broker 服务协议别名</span></span><br><span class="line"><span class="attr">inter.broker.listener.name</span>=<span class="string">PLAINTEXT</span></span><br><span class="line"><span class="comment">#broker 对外暴露的地址</span></span><br><span class="line"><span class="attr">advertised.Listeners</span>=<span class="string">PLAINTEXT://hadoop102:9092</span></span><br><span class="line"><span class="comment">#协议别名到安全协议的映射</span></span><br><span class="line"><span class="attr">listener.security.protocol.map</span>=<span class="string">CONTROLLER:PLAINTEXT,PLAINTEXT:PLA</span></span><br><span class="line"><span class="attr">INTEXT,SSL</span>:<span class="string">SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL</span></span><br><span class="line"><span class="comment">#kafka 数据存储目录</span></span><br><span class="line"><span class="attr">log.dirs</span>=<span class="string">/opt/kafka2/data</span></span><br></pre></td></tr></table></figure>

<p>初始化集群数据目录：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">首先生成存储目录唯一 ID。</span></span><br><span class="line">[nanzx kafka2]$ bin/kafka-storage.sh random-uuid</span><br><span class="line">J7s9e8PPTKOO47PxzI39VA</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">用该 ID 格式化 kafka 存储目录（三台节点）。</span></span><br><span class="line">[nanzx kafka2]$ bin/kafka-storage.sh format -t J7s9e8PPTKOO47PxzI39VA -c </span><br><span class="line">/opt/kafka2/config/kraft/server.properties</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动 kafka 集群</span></span><br><span class="line">[nanzx kafka2]$ bin/kafka-server-start.sh -daemon config/kraft/server.properties</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">停止 kafka 集群</span></span><br><span class="line">[nanzx kafka2]$ bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure>

<h1 id="Kafka整合SpringBoot-Demo"><a href="#Kafka整合SpringBoot-Demo" class="headerlink" title="Kafka整合SpringBoot Demo"></a>Kafka整合SpringBoot Demo</h1><p>引入依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-kafka<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-kafka-test<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>修改配置文件：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 应用名称</span></span><br><span class="line"><span class="attr">spring.application.name</span>=<span class="string">Learn_Kafka</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 生产者配置</span></span><br><span class="line"><span class="attr">spring.kafka.bootstrap-servers</span>=<span class="string">192.168.2.110:9092</span></span><br><span class="line"><span class="attr">spring.kafka.producer.key-serializer</span>=<span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br><span class="line"><span class="attr">spring.kafka.producer.value-serializer</span>=<span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#消费者配置</span></span><br><span class="line"><span class="attr">spring.kafka.consumer.key-deserializer</span>=<span class="string">org.apache.kafka.common.serialization.StringDeserializer</span></span><br><span class="line"><span class="attr">spring.kafka.consumer.value-deserializer</span>=<span class="string">org.apache.kafka.common.serialization.StringDeserializer</span></span><br><span class="line"><span class="attr">spring.kafka.consumer.group-id</span>=<span class="string">nanzx</span></span><br></pre></td></tr></table></figure>

<p>生产者：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyProducer</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> KafkaTemplate&lt;String,String&gt; kafkaTemplate;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@GetMapping(&quot;/producer/&#123;message&#125;&quot;)</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">sendMessage1</span><span class="params">(<span class="meta">@PathVariable(&quot;message&quot;)</span> String normalMessage)</span> &#123;</span><br><span class="line">        kafkaTemplate.send(<span class="string">&quot;firstTopic&quot;</span>, normalMessage);</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;OK&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>消费者：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyConsumer</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@KafkaListener(topics = &quot;firstTopic&quot;)</span></span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">consume</span><span class="params">(ConsumerRecord&lt;?, ?&gt; record)</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;简单消费：&quot;</span> + record.topic() + <span class="string">&quot;-&quot;</span> + record.partition() + <span class="string">&quot;-&quot;</span> + record.value());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>SpringBoot集成kafka全面实战：</strong><a target="_blank" rel="noopener" href="https://blog.csdn.net/yuanlong122716/article/details/105160545/">https://blog.csdn.net/yuanlong122716/article/details/105160545/</a></p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://nanzx.gitee.io">阿楠</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://nanzx.gitee.io/posts/13749/">https://nanzx.gitee.io/posts/13749/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://nanzx.gitee.io" target="_blank">Nan</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Kafka/">Kafka</a></div><div class="post_share"><div class="social-share" data-image="https://unpkg.com/nan-picture/img/wp7.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://unpkg.com/nan-picture/img/微信收款码.jpg" target="_blank"><img class="post-qr-code-img" src="https://unpkg.com/nan-picture/img/微信收款码.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://unpkg.com/nan-picture/img/支付宝收款码.jpg" target="_blank"><img class="post-qr-code-img" src="https://unpkg.com/nan-picture/img/支付宝收款码.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/38577/" title="总结"><img class="cover" src="https://unpkg.com/nan-picture/img/wp8.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">总结</div></div></a></div><div class="next-post pull-right"><a href="/posts/27273/" title="Redis"><img class="cover" src="https://unpkg.com/nan-picture/img/wp6.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Redis</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-number">1.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">1.2.</span> <span class="toc-text">应用场景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.3.</span> <span class="toc-text">消息队列的两种模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84"><span class="toc-number">1.4.</span> <span class="toc-text">基础架构</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%A5%E9%97%A8"><span class="toc-number">2.</span> <span class="toc-text">入门</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2"><span class="toc-number">2.1.</span> <span class="toc-text">安装部署</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8%E5%92%8C%E5%81%9C%E6%AD%A2"><span class="toc-number">2.2.</span> <span class="toc-text">启动和停止</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C"><span class="toc-number">2.3.</span> <span class="toc-text">命令行操作</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka-%E7%94%9F%E4%BA%A7%E8%80%85"><span class="toc-number">3.</span> <span class="toc-text">Kafka 生产者</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E6%B5%81%E7%A8%8B"><span class="toc-number">3.1.</span> <span class="toc-text">消息发送流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0%E5%88%97%E8%A1%A8"><span class="toc-number">3.2.</span> <span class="toc-text">生产者重要参数列表</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%82%E6%AD%A5%E5%8F%91%E9%80%81API"><span class="toc-number">3.3.</span> <span class="toc-text">异步发送API</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%8C%E6%AD%A5%E5%8F%91%E9%80%81API"><span class="toc-number">3.4.</span> <span class="toc-text">同步发送API</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5"><span class="toc-number">3.5.</span> <span class="toc-text">分区策略</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E7%BB%8F%E9%AA%8C"><span class="toc-number">3.6.</span> <span class="toc-text">生产经验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%90%E9%AB%98%E5%90%9E%E5%90%90%E9%87%8F"><span class="toc-number">3.6.1.</span> <span class="toc-text">提高吞吐量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8F%AF%E9%9D%A0%E6%80%A7%EF%BC%88ACK%E5%BA%94%E7%AD%94%E6%9C%BA%E5%88%B6%EF%BC%89"><span class="toc-number">3.6.2.</span> <span class="toc-text">数据可靠性（ACK应答机制）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8E%BB%E9%87%8D%EF%BC%88%E5%B9%82%E7%AD%89%E6%80%A7%E5%92%8C%E4%BA%8B%E5%8A%A1%EF%BC%89"><span class="toc-number">3.6.3.</span> <span class="toc-text">数据去重（幂等性和事务）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%9C%89%E5%BA%8F"><span class="toc-number">3.6.4.</span> <span class="toc-text">数据有序</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka-Broker"><span class="toc-number">4.</span> <span class="toc-text">Kafka Broker</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Broker-%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-number">4.1.</span> <span class="toc-text">Broker 工作流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Broker-%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0%E5%88%97%E8%A1%A8"><span class="toc-number">4.2.</span> <span class="toc-text">Broker 重要参数列表</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka-%E5%89%AF%E6%9C%AC"><span class="toc-number">4.3.</span> <span class="toc-text">Kafka 副本</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Leader-%E5%92%8C-Follower-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E7%BB%86%E8%8A%82"><span class="toc-number">4.3.1.</span> <span class="toc-text">Leader 和 Follower 故障处理细节</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E5%89%AF%E6%9C%AC%E5%88%86%E9%85%8D"><span class="toc-number">4.3.2.</span> <span class="toc-text">分区副本分配</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E8%B0%83%E6%95%B4%E5%88%86%E5%8C%BA%E5%89%AF%E6%9C%AC%E5%AD%98%E5%82%A8"><span class="toc-number">4.3.3.</span> <span class="toc-text">手动调整分区副本存储</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Leader-Partition%E8%87%AA%E5%8A%A8%E5%B9%B3%E8%A1%A1"><span class="toc-number">4.3.4.</span> <span class="toc-text">Leader Partition自动平衡</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6"><span class="toc-number">4.4.</span> <span class="toc-text">文件存储机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E6%B8%85%E7%90%86%E7%AD%96%E7%95%A5"><span class="toc-number">4.5.</span> <span class="toc-text">文件清理策略</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AB%98%E6%95%88%E8%AF%BB%E5%86%99%E6%95%B0%E6%8D%AE"><span class="toc-number">4.6.</span> <span class="toc-text">高效读写数据</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka-%E6%B6%88%E8%B4%B9%E8%80%85"><span class="toc-number">5.</span> <span class="toc-text">Kafka 消费者</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka-%E6%B6%88%E8%B4%B9%E6%96%B9%E5%BC%8F"><span class="toc-number">5.1.</span> <span class="toc-text">Kafka 消费方式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84"><span class="toc-number">5.2.</span> <span class="toc-text">消费者组</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E5%88%9D%E5%A7%8B%E5%8C%96%E6%B5%81%E7%A8%8B"><span class="toc-number">5.2.1.</span> <span class="toc-text">消费者组初始化流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E8%AF%A6%E7%BB%86%E6%B6%88%E8%B4%B9%E6%B5%81%E7%A8%8B"><span class="toc-number">5.2.2.</span> <span class="toc-text">消费者组详细消费流程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0%E5%88%97%E8%A1%A8"><span class="toc-number">5.3.</span> <span class="toc-text">消费者重要参数列表</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85API"><span class="toc-number">5.4.</span> <span class="toc-text">消费者API</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E7%9A%84%E5%88%86%E9%85%8D%E4%BB%A5%E5%8F%8A%E5%86%8D%E5%B9%B3%E8%A1%A1"><span class="toc-number">5.5.</span> <span class="toc-text">分区的分配以及再平衡</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Range-%E4%BB%A5%E5%8F%8A%E5%86%8D%E5%B9%B3%E8%A1%A1"><span class="toc-number">5.5.1.</span> <span class="toc-text">Range 以及再平衡</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RoundRobin-%E4%BB%A5%E5%8F%8A%E5%86%8D%E5%B9%B3%E8%A1%A1"><span class="toc-number">5.5.2.</span> <span class="toc-text">RoundRobin 以及再平衡</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sticky-%E4%BB%A5%E5%8F%8A%E5%86%8D%E5%B9%B3%E8%A1%A1"><span class="toc-number">5.5.3.</span> <span class="toc-text">Sticky 以及再平衡</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#offset%E4%BD%8D%E7%A7%BB"><span class="toc-number">5.6.</span> <span class="toc-text">offset位移</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%8F%90%E4%BA%A4-offset"><span class="toc-number">5.6.1.</span> <span class="toc-text">自动提交 offset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E6%8F%90%E4%BA%A4-offset"><span class="toc-number">5.6.2.</span> <span class="toc-text">手动提交 offset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E5%AE%9A-Offset-%E6%B6%88%E8%B4%B9"><span class="toc-number">5.6.3.</span> <span class="toc-text">指定 Offset 消费</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E5%AE%9A%E6%97%B6%E9%97%B4%E6%B6%88%E8%B4%B9"><span class="toc-number">5.6.4.</span> <span class="toc-text">指定时间消费</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BC%8F%E6%B6%88%E8%B4%B9%E5%92%8C%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9"><span class="toc-number">5.7.</span> <span class="toc-text">漏消费和重复消费</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E7%A7%AF%E5%8E%8B"><span class="toc-number">5.8.</span> <span class="toc-text">数据积压</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka%E7%9B%91%E6%8E%A7"><span class="toc-number">6.</span> <span class="toc-text">Kafka监控</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"><span class="toc-number">6.1.</span> <span class="toc-text">其他环境准备</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka-Eagle-%E5%AE%89%E8%A3%85"><span class="toc-number">6.2.</span> <span class="toc-text">Kafka-Eagle 安装</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka-Kraft-%E6%A8%A1%E5%BC%8F"><span class="toc-number">7.</span> <span class="toc-text">Kafka-Kraft 模式</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84"><span class="toc-number">7.1.</span> <span class="toc-text">架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2-1"><span class="toc-number">7.2.</span> <span class="toc-text">安装部署</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka%E6%95%B4%E5%90%88SpringBoot-Demo"><span class="toc-number">8.</span> <span class="toc-text">Kafka整合SpringBoot Demo</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By 阿楠</div><div class="footer_custom_text">世间总有赏不尽的繁华，也有尝不完的辛苦，若内心旖旎，穷山恶水也会云卷云舒。</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://twikoo.nanzx.top/',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://twikoo.nanzx.top/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.textContent = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getComment = () => {
    const runTwikoo = () => {
      twikoo.getRecentComments({
        envId: 'https://twikoo.nanzx.top/',
        region: '',
        pageSize: 3,
        includeReply: true
      }).then(function (res) {
        const twikooArray = res.map(e => {
          return {
            'content': changeContent(e.comment),
            'avatar': e.avatar,
            'nick': e.nick,
            'url': e.url + '#' + e.id,
            'date': new Date(e.created).toISOString()
          }
        })

        saveToLocal.set('twikoo-newest-comments', JSON.stringify(twikooArray), 10/(60*24))
        generateHtml(twikooArray)
      }).catch(function (err) {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.textContent= "无法获取评论，请确认相关配置是否正确"
      })
    }

    if (typeof twikoo === 'object') {
      runTwikoo()
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runTwikoo)
    }
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (false) {
          const name = 'data-lazy-src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'></a>`
        }
        
        result += `<div class='content'>
        <a class='comment' href='${array[i].url}' title='${array[i].content}'>${array[i].content}</a>
        <div class='name'><span>${array[i].nick} / </span><time datetime="${array[i].date}">${btf.diffDate(array[i].date, true)}</time></div>
        </div></div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('twikoo-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><div class="aplayer no-destroy" data-id="7500003406" data-server="netease" data-type="playlist" data-fixed="true" data-mini="true" data-listFolded="false" data-order="random" data-preload="auto" data-autoplay="true" data-lrctype="0"></div><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>